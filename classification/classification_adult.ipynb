{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e8d3e8-8b5b-402b-a46e-ecaa31b1b7bd",
   "metadata": {},
   "source": [
    "# **Classification Practical Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcb10d-a306-44aa-8ee7-940e4608d515",
   "metadata": {},
   "source": [
    "## **Datasets overview and preliminary analysis**\n",
    "\n",
    "In this lab, we explore two datasets: the **adult dataset** and the **student dataset**. The adult dataset is related to demographics and income levels, while the student dataset focuses on student performance and characteristics. These datasets belong to the social and education domains, respectively. The goal is to analyze and model patterns within the data, such as understanding income brackets or predicting academic outcomes. The datasets include a mix of attribute types, such as numerical (e.g., age, scores) and categorical data (e.g., occupation, gender).\n",
    "\n",
    "The **adult** and **student** data are first loaded into `DataFrame` data structures from the `adult.csv` and `student.csv` files. When working with data in Python, these packages are very helpful for different tasks:  \n",
    "\n",
    "1. **`sys`**: This package helps manage Python's environment. It lets you interact with the system, such as reading command-line arguments or exiting a program.  \n",
    "2. **`numpy` (`np`)**: A powerful library for working with numbers. It makes it easy to perform mathematical operations on large arrays or tables of data.  \n",
    "3. **`pandas` (`pd`)**: A library for working with `DataFrame` structures. It helps organize, analyze, and clean data efficiently.  \n",
    "4. **`matplotlib.pyplot` (`plt`)**: A tool for creating graphs and charts to visualize data.  \n",
    "5. **`seaborn` (`sns`)**: Built on top of `matplotlib`, it simplifies the process of creating beautiful and advanced visualizations.  \n",
    "6. **`adsa_utils2` (`ad`)**: A custom Python set of functions useful in the context of the assessment labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac783c-3249-48bb-ab21-a5a7b5b830c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line sets the filter for warnings. It tells Python to ignore all warnings that are generated during the execution of the program.\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import essentil libraries \n",
    "import sys\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "import ads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fce73e-2f74-47e0-b5fa-dfb484b40bd3",
   "metadata": {},
   "source": [
    "### **Adult dataset**\n",
    "\n",
    "The adult dataset is a collection of data related to **income** and **demographics**, primarily used to understand factors influencing income levels. The dataset is from the socioeconomic domain, focusing on **identifying individuals earning more than $50,000 annually**. This analysis helps in understanding **income disparities**, **labor market trends**, and **demographic characteristics affecting earnings**, making it useful for social research and policy-making.  \n",
    "\n",
    "The dataset contains 947 rows with a binary class label (`<=50K`, `>50K`) indicating income levels. It includes a diverse set of attributes, comprising **6 numerical** and **8 categorical** data types. For example, continuous attributes like age, capital gains/losses, and hours worked per week provide quantitative insights, while categorical features such as workclass, education level, occupation, and marital status reflect demographic and professional characteristics.  \n",
    "\n",
    "The dataset also includes information about individuals' native countries, spanning diverse regions such as the United States, Europe, Asia, and Latin America. This allows us to **explore how geographic and cultural backgrounds influence income levels**, adding a layer of complexity to the analysis.  \n",
    "\n",
    "This dataset is frequently used in **predictive modeling for income classification**, helping to uncover how factors like education, work hours, and occupation impact earning potential. The mix of numerical and categorical data presents challenges during preprocessing, such as encoding categories into numerical formats and addressing potential imbalances in class labels (`<=50K` and `>50K`). By analyzing and modeling this data, we gain valuable insights into the **interplay of various demographic, geographic, and professional factors** that contribute to income disparities in diverse populations poses challenges for preprocessing, including encoding categorical variables and handling potential imbalances in the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c3192-2436-46f4-a16a-58cbc9aa7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data= pd.read_csv('data/adult.csv', sep=';')\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3fc07-01d5-484b-b50b-4e59265d9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae75ab-1b09-4baf-ad52-85b5c923dfba",
   "metadata": {},
   "source": [
    "All columns have **947 non-null values**, confirming that there are no missing entries. This is beneficial for analysis as it eliminates the need for data imputation or handling missing values. Additionally, some categorical attributes will be primarily analyzed based on their unique values to clarify ambiguities and identify the distinct categories or groups within each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c5f63-db80-4718-b022-51a562427379",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08660c25-dffc-4e02-9249-8aa02021f04c",
   "metadata": {},
   "source": [
    "The output of `adult_data['education'].unique()` shows the unique values in the \"education\" column. These values represent different levels or types of education that individuals in the dataset have completed. Here's a brief analysis of these values.\n",
    "\n",
    "The education levels are categorized across different stages of formal schooling. **Primary education** typically includes **1st-6th grade**, with the dataset reflecting this in categories like **1st-4th** and **5th-6th**. **Secondary education** spans **7th-12th grade**, which includes **middle school** (grades 7-8) and **high school** (grades 9-12). The dataset reflects this progression with categories like **7th-8th**, **9th**, **10th**, **11th**, and **12th**. **Higher education** follows high school and includes degrees like **Associate**, **Bachelor’s**, **Master’s**, and **Doctorate**.\n",
    "\n",
    "Additionally, **Some-college** refers to individuals who have attended college but have not completed a degree, typically after high school, while **Preschool** refers to early childhood education. Categories like **HS-grad** indicate high school graduates, and **Assoc-acdm** and **Assoc-voc** represent associate degrees, typically completed after high school, with **Assoc-acdm** focusing on academic subjects and **Assoc-voc** on vocational training. **Prof-school** represents education from specialized schools, such as law or medical schools, often following a bachelor's degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7325f97-c553-4f6c-9416-cd4870c54b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data['occupation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab9750-ea66-4d76-b2ab-5f43460fa0f4",
   "metadata": {},
   "source": [
    "The **occupation** attribute in the dataset represents various job categories, such as managerial roles (Exec-managerial), technical support (Tech-support), skilled trades (Craft-repair), and professional specialties (Prof-specialty). It also includes roles in sales, administrative work (Adm-clerical), and protective services (Protective-serv). Some categories, like **Other-service** and **Priv-house-serv**, are more ambiguous, as they are broad and lack specific details, with **Other-service** covering various unspecified service roles and **Priv-house-serv** referring to household workers without further clarification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280a011-5999-4d95-9084-a3560fb092ec",
   "metadata": {},
   "source": [
    "## **Classification models for Adult dataset**\n",
    "\n",
    "### **Algorithm choices**\n",
    "\n",
    "In this analysis, the goal is to compare different classification algorithms for predicting income levels in the Adult dataset, where the target variable is a binary class label. The first model chosen is **logistic regression**, a simple and interpretable model designed for **binary classification**. Although logistic regression is a form of regression, it is widely used for classification tasks where the outcome is a binary class. It is effective when the relationship between the features and the target is mostly linear and provides clear insights into how each feature impacts the model's prediction. This simplicity and interpretability make logistic regression a natural choice as a baseline model for comparison.\n",
    "\n",
    "The second approach involves **decision trees**, followed by **XGBoost**, a powerful gradient boosting algorithm. Decision trees are able to handle both numerical and categorical data, and can capture non-linear relationships and interactions between features. However, their performance can often be limited by overfitting or underfitting. To address this, **XGBoost** is applied to enhance the decision tree model. As an ensemble method, XGBoost combines multiple decision trees and is particularly effective at managing complex, non-linear relationships, leading to improved accuracy and robustness.\n",
    "\n",
    "### **Logistic regression model**\n",
    "\n",
    "Logistic regression is a classification algorithm used for predicting a binary class label (e.g., 0 or 1). It is based on the concept of estimating the probability that an instance (row in the dataset) belongs to a particular class.\n",
    "\n",
    "- **Logistic function (Sigmoid)**: The core of logistic regression is the **sigmoid function**, which transforms any real-valued number into a probability between 0 and 1. It is given by:\n",
    "$$\n",
    "P(y=1 | X) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "where $P(y=1 | X)$ is the probability that the instance belongs to class 1, and $z$ is the **logit**, which is a linear combination of the input features and their corresponding coefficients:\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "where $\\beta_0$ is the intercept term, and $\\beta_1, \\beta_2, \\dots, \\beta_n$ are the coefficients for the features $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "- **Decision rule**:\n",
    "To classify an instance, the output probability is compared to a threshold (usually 0.5):\n",
    "    - If $P(y=1 | X) \\geq 0.5$, the instance is classified as **class 1**.\n",
    "    - If $P(y=1 | X) < 0.5$, the instance is classified as **class 0**.\n",
    "\n",
    "- **Cost function**:\n",
    "Logistic regression uses a cost function called **log-loss** or **binary cross-entropy** to measure how well the model's predictions match the actual class labels. The cost function is minimized during training:\n",
    "$$\n",
    "J(\\beta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log(h_\\beta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\beta(x^{(i)})) \\right)\n",
    "$$\n",
    "where $m$ is the number of training examples, $y^{(i)}$ is the actual label of the $i$-th training example, and $h_\\beta(x^{(i)})$ is the predicted probability for the $i$-th example.\n",
    "\n",
    "Before applying logistic regression, **categorical features** in the dataset must be transformed into numerical representations using techniques such as **one-hot encoding**, **label encoding**, or **ordinal encoding**. One-hot encoding is often preferred because it prevents the model from assuming any order or hierarchy among the categories, which label encoding or ordinal encoding might unintentionally introduce. Logistic regression is a linear model, and it interprets numerical inputs as having a specific magnitude or order. If categories are encoded as integers using label encoding, the model could mistakenly assume that higher numbers represent greater importance or imply a ranking, even when no such relationship exists. Similarly, ordinal encoding explicitly assigns a ranking to categories (e.g., `Low < Medium < High`), which is only appropriate when the categories naturally have an order. However, using ordinal encoding for nominal data (e.g., `Red`, `Green`, `Blue`) would also lead to incorrect assumptions about relationships between the categories. One-hot encoding solves these issues by creating separate binary columns for each category, treating them as independent and ensuring the model does not infer relationships that do not exist. This makes one-hot encoding more suitable for preserving the integrity of categorical data when applying logistic regression, especially when the categories lack a meaningful order.\n",
    "\n",
    "In addition to transforming the data properly, the evaluation of logistic regression should not rely solely on the cost function, such as log-loss, but also consider a variety of metrics derived from the confusion matrix. Metrics like **accuracy**, **precision**, **recall**, and **F1 score** provide detailed insights into the model’s classification performance. The **ROC-AUC** metric is especially important for understanding how well the model distinguishes between the two classes across different thresholds. For imbalanced datasets, other metrics, such as **Cohen’s Kappa** and the **Geometric Mean (G-Mean)**, are helpful as they account for imbalances in the class distribution and provide a more nuanced evaluation. These metrics help ensure the model's predictions are assessed comprehensively, capturing both its strengths and weaknesses.\n",
    "\n",
    "With these preparations completed, the next step will involve the preparation of the dataset for training and conducting an analysis using Python to evaluate its performance and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18dda3d-f266-404c-9632-97b8268ccc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of adult_data\n",
    "ad = adult_data.copy()\n",
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e0d7e-898c-4438-98f5-8263627f3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder for categorical attributes\n",
    "# onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids the dummy variable trap\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)  # drop='first' avoids the dummy variable trap\n",
    "\n",
    "# Initialize the OrdinalEncoder for the label column\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Identify categorical columns (excluding the label column)\n",
    "ad_c = ad.select_dtypes(include=['object']).columns\n",
    "\n",
    "# If the label column is categorical, apply OrdinalEncoder to it\n",
    "ad['label'] = ordinal_encoder.fit_transform(ad[['label']])\n",
    "\n",
    "# Remove the label from the categorical list\n",
    "ad_c = ad_c[ad_c != 'label']\n",
    "\n",
    "# Apply one-hot encoding to the categorical features (excluding the label column)\n",
    "ad_c_e = onehot_encoder.fit_transform(ad[ad_c])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "ad_c_e_df = pd.DataFrame(ad_c_e, columns=onehot_encoder.get_feature_names_out(ad_c))\n",
    "\n",
    "# Drop the original categorical columns (including label) and concatenate the encoded ones\n",
    "ad_e = pd.concat([ad.drop(columns=ad_c), ad_c_e_df], axis=1)\n",
    "\n",
    "# Display the updated data\n",
    "ad_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b2059-4dba-4805-8478-4adbc9bb3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Manually list the numerical columns\n",
    "ad_n = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']  # replace with actual names of numerical columns\n",
    "\n",
    "ad_e_s = ad_e.copy()\n",
    "\n",
    "# Apply scaling to numerical columns\n",
    "ad_e_s[ad_n] = scaler.fit_transform(ad_e[ad_n])\n",
    "\n",
    "# Display the updated data with scaled numerical features\n",
    "ad_e_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d492ea-9577-445d-bb3a-40f2043112f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ad_e_s.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a8c95-ff36-4585-969a-a4dc9546c46c",
   "metadata": {},
   "source": [
    "When working with small datasets, it's often better to use **cross-validation** instead of **train-test split validation**. With split validation, the data is divided into just two sets: one for training and one for testing. This approach can leave some data unused for training, which may result in an unrepresentative model, especially with small datasets.\n",
    "\n",
    "In contrast, cross-validation splits the data into multiple smaller parts (folds) and uses each part for both training and testing. This ensures that the model is trained and tested on all available data, leading to better performance and more reliable evaluation. Cross-validation is, therefore, a better approach when working with limited data.\n",
    "\n",
    "However, before applying any model, we should always **split** the dataset into **training** and **testing** sets. This step ensures that the model is evaluated on **unseen data**, providing a better understanding of its real-world performance. Once the initial split is done, **cross-validation** can be applied to the training set to optimize and fine-tune the model. Afterward, the model can be tested on the **test set** to assess its ability to generalize.\n",
    "\n",
    "This approach is beneficial for two main reasons:\n",
    "- It allows for **model optimization** using cross-validation on the training set, reducing the risk of overfitting.\n",
    "- It provides an accurate measure of the model's performance on unseen data (the test set), helping us evaluate how it might perform in real-world applications.\n",
    "\n",
    "Since our dataset is relatively small, we will use a **0.3 test / 0.7 train split**. This will give us a larger test set, which is important for evaluating the model's performance more reliably. With a larger test set, we can better assess how well the model generalizes to unseen data and ensure that the performance metrics we calculate are more robust and meaningful. This approach helps prevent overfitting and gives a clearer picture of how the model will perform in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5bbf3-9d96-4b09-b69e-278202f617bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ad_e_s = ad_e_s[ad_e_s.columns.difference(['label'])]\n",
    "print(X_ad_e_s.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360177d8-0d80-4fc6-b6ba-99c3ce22c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ad_e_s = ad_e_s['label'] \n",
    "y_ad_e_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2a7b4-66a1-405f-ad75-80c0a74354c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_ad_e_s_train, X_ad_e_s_test, y_ad_e_s_train, y_ad_e_s_test = train_test_split(X_ad_e_s, y_ad_e_s,\n",
    "                                                                                test_size=0.3,\n",
    "                                                                                stratify=y_ad_e_s,\n",
    "                                                                                random_state=43)\n",
    "# `stratify=y_ad`: This ensures that the distribution of the target variable (`y_ad`) is maintained in both the training and test sets.\n",
    "\n",
    "# Define the model (e.g., Logistic Regression)\n",
    "logistic_regression_m_ad = LogisticRegression()\n",
    "\n",
    "ads.custom_crossvalidation(X_ad_e_s_train, y_ad_e_s_train, logistic_regression_m_ad, cv_=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e2f45-95d9-43de-a1f7-03b3f61d8222",
   "metadata": {},
   "source": [
    "Logistic Regression is a widely used machine learning algorithm for binary and multi-class classification tasks. However, its performance heavily depends on the choice of **hyperparameters**, such as:\n",
    "\n",
    "1. **Regularization strength (`C`)**: Controls the trade-off between fitting the training data and regularization to prevent overfitting.\n",
    "2. **Penalty type (`l1`, `l2`, etc.)**: Determines the kind of regularization applied to the model.\n",
    "\n",
    "Manual selection of these hyperparameters is time-consuming and prone to suboptimal results. **GridSearchCV** automates this process by systematically exploring combinations of hyperparameter values and selecting the combination that maximizes model performance, based on a chosen metric (e.g., accuracy, F1 score).\n",
    "\n",
    "Using GridSearch:\n",
    "1. **Optimizes model performance**: Ensures that the best combination of hyperparameters is chosen for the given dataset.\n",
    "2. **Reduces human bias**: Removes guesswork and ensures a more objective selection process.\n",
    "3. **Efficient evaluation**: Uses cross-validation to test each combination, providing a reliable estimate of performance.\n",
    "\n",
    "By applying GridSearch, we ensure the Logistic Regression model is not only accurate but also generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a8597-d2e7-477d-9d6a-7eff9d503951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expanded parameter grid for Logistic Regression\n",
    "logistic_regression_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength (larger range for flexibility)\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization types (L1, L2, or ElasticNet)\n",
    "    'solver': ['liblinear', 'saga', 'newton-cg', 'lbfgs'],  # Optimization algorithms\n",
    "    'max_iter': [100, 200],  # Maximum number of iterations for optimization\n",
    "    'tol': [1e-4, 1e-3, 1e-2],  # Tolerance for stopping criteria\n",
    "    'fit_intercept': [True, False],  # Whether to include an intercept in the model\n",
    "    'class_weight': [None, 'balanced']  # Class weight options (useful for imbalanced datasets)\n",
    "}\n",
    "\n",
    "# Call the custom_crossvalidation function\n",
    "best_logistic_regression_m_ad, best_logistic_regression_m_ad_params, best_logistic_regression_m_ad_preds = (\n",
    "    ads.custom_crossvalidation_gridsearch(\n",
    "        X_ad_e_s_train, \n",
    "        y_ad_e_s_train, \n",
    "        logistic_regression_m_ad, \n",
    "        logistic_regression_param_grid, \n",
    "        cv_=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007023f-5c08-4fa4-a90b-a1e8a9c939bc",
   "metadata": {},
   "source": [
    "Accuracy measures how many predictions were correct overall. It is calculated by dividing the number of correct predictions (true positives and true negatives) by the total number of instances:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "In this case, the accuracy is **82.62%**, meaning the model correctly predicted the class for about 83% of the instances. While useful, accuracy can be misleading when the dataset is imbalanced because it may overlook the model's performance on the minority class.\n",
    "\n",
    "Precision measures how many of the predicted positive instances are actually correct. It is calculated as the number of true positives (TP) divided by the sum of true positives and false positives (FP):\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "For the <=50K class, the precision is 86%, meaning most predictions for this class are correct. For the >50K class, the precision is **80%**, indicating that a smaller proportion of predicted >50K instances were correct. Precision is important when the cost of false positives is high.\n",
    "\n",
    "Recall shows how well the model identifies all the actual positive instances. It is calculated as the number of true positives (TP) divided by the sum of true positives and false negatives (FN):\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "For <=50K, recall is 78%, meaning the model missed about 22% of the instances for this class. For >50K, recall is 88%, meaning the model identified most of the >50K instances. Recall is crucial when it is important to identify as many positives as possible.\n",
    "\n",
    "The F1-score is the harmonic mean of precision and recall, providing a balance between them. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{F1-score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "For the <=50K class, the F1-score is **81%**, and for the >50K class, it is **84%**. This indicates that the model balances precision and recall well for both classes, with a slightly better performance for >50K.\n",
    "\n",
    "Other metrics like support, macro average, and weighted average are less informative for this binary classification. Support simply counts the number of instances in each class, and averages like macro or weighted averages are more useful for imbalanced datasets or multi-class problems. \n",
    "\n",
    "The output suggests that the model achieved a mean accuracy of 82.62% with a small standard deviation (3.94%), indicating a good general performance with slight variability across folds. The precision for both classes is good (83.06% for <=50K and 80% for >50K), but there is a trade-off between precision and recall, with recall being slightly lower for <=50K (78%). The F1-score, which balances both precision and recall, is around 82.55%, indicating that the model is well-balanced in terms of both metrics.\n",
    "\n",
    "These metrics can be used to further tune the model for better performance, especially in handling imbalanced datasets by adjusting parameters like class_weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec725f6a-d5d7-4cbf-9959-37d1b36b36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you apply the same technique to transform the test data\n",
    "# then apply the model trained above to the test portion\n",
    "y_ad_e_s_test_predicted = best_logistic_regression_m_ad.predict(X_ad_e_s_test)\n",
    "print(classification_report(y_ad_e_s_test, y_ad_e_s_test_predicted))\n",
    "ads.plot_confusion_matrix(y_ad_e_s_test, y_ad_e_s_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ebc5a-109c-4c45-a6ac-31067ff302c8",
   "metadata": {},
   "source": [
    "**Predictions Confidence Levels**\n",
    "\n",
    "\n",
    "- In scikit-learn, once we train a model, we can retrieve the confidence levels (also called probabilities) of a classifier's predictions using the predict_proba method which returns an array with the probabilities of each class for each row/instance.\n",
    "- This is especially useful for measuring the model's confidence in its predictions.\n",
    "\n",
    "NOTE: Most classifiers in scikit-learn support predict_proba, but a few don't, such as SVM (Support Vector Machine) which does not directly support probabilities, but we can enable probability estimates by setting probability=True when initializing the SVC model (note that this adds computational cost).\n",
    "\n",
    "NOTE: adding the predictions and confidence levels to the test dataset is implemented in get_test_dataset from adsa_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78108be8-8fae-4440-abaa-4fa125b053b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit (train) the model\n",
    "best_logistic_regression_m_ad.fit(X_ad_e_s_train, y_ad_e_s_train)\n",
    "# call the function get_test_dataset from adsa_utils\n",
    "ads.get_test_dataset(best_logistic_regression_m_ad, X_ad_e_s_train, y_ad_e_s_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6a470-de86-4f12-a1a1-85ecba95603d",
   "metadata": {},
   "source": [
    "### 2.3 **Decision tree models**\n",
    "\n",
    "Decision trees are widely recognized as effective supervised learning tools for classification and regression. Data is divided into subsets based on specific rules, forming a tree-like structure where **leaf nodes** represent final decisions. Measures such as **entropy** or the **Gini index** are used to evaluate the quality of splits, ensuring meaningful partitions. However, if trees are grown excessively deep, **overfitting** can occur, where noise is captured instead of general patterns.  \n",
    "\n",
    "After logistic regression has been applied to the Adult dataset, the **C5.0 algorithm** will be used for further analysis and classification. This advanced decision tree algorithm is designed to improve upon its predecessor, C4.5, by incorporating **boosting**, which allows weak models to be combined for higher accuracy. Continuous attributes and missing data are handled efficiently, while **post-pruning** is employed to simplify the tree and reduce overfitting. The tree is first grown fully, and branches that do not significantly enhance accuracy are removed, resulting in a model that is both interpretable and effective for unseen data.\n",
    "\n",
    "By applying the C5.0 algorithm, improved classification performance and deeper insights into the Adult dataset are expected to be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1bb67-80db-44be-859e-a45a70e7e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = adult_data.copy()\n",
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0d6c3-50af-460b-bc4d-f4beba60e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ad = ad[ad.columns.difference(['label'])]\n",
    "X_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cf225-8245-4c27-bf4b-aefe2cbbad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ad = ad[['label']] \n",
    "y_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29eb96d-077b-4451-9fa4-a8a5c51a8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Identify categorical columns\n",
    "X_ad_c = X_ad.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply ordinal encoding to categorical columns\n",
    "X_ad_c_e = encoder.fit_transform(X_ad[X_ad_c])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "X_ad_c_e = pd.DataFrame(X_ad_c_e, columns=encoder.get_feature_names_out(X_ad_c))\n",
    "\n",
    "# Drop the original categorical columns and concatenate the encoded ones\n",
    "X_ad_e = pd.concat([X_ad.drop(columns=X_ad_c), X_ad_c_e], axis=1)\n",
    "\n",
    "# Display the updated data\n",
    "X_ad_e.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3861abc-8874-4896-9d0d-d8e1e8b33b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_ad_e_train, X_ad_e_test, y_ad_train, y_ad_test = train_test_split(X_ad_e, y_ad,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    stratify=y_ad,\n",
    "                                                                    random_state=43)\n",
    "# `stratify=y_ad`: This ensures that the distribution of the target variable (`y_ad`) is maintained in both the training and test sets.\n",
    "\n",
    "# Define the model (e.g., decision tree classifier)\n",
    "decision_tree_classifier_m_ad = DecisionTreeClassifier(random_state=43)\n",
    "\n",
    "ads.custom_crossvalidation(X_ad_e_train, y_ad_train, decision_tree_classifier_m_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1467e-e404-4c7a-bca0-6ff56d78153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tree model\n",
    "decision_tree_classifier_m_ad.fit(X_ad_e_train, y_ad_train)\n",
    "plt.figure(figsize=(25, 16))\n",
    "plot_tree(decision_tree_classifier_m_ad, \n",
    "          filled=True, \n",
    "          feature_names=list(X_ad_e_train.columns), \n",
    "          class_names=decision_tree_classifier_m_ad.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02d3f1-81e2-43d1-8291-0c932aafd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can output the depth and the number of leaf nodes\n",
    "decision_tree_classifier_m_ad.get_depth(), decision_tree_classifier_m_ad.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382907ea-205f-4d07-b7ac-32b58c1dfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decision_tree_classifier_m_ad.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dbfc7-422a-40ac-9d1f-7738192f937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad.decision_path(X_ad_e_train, check_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687b3f0-fd0a-4b67-94d1-01b6239e8d91",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "we can manually modify various hyperparameters:\n",
    "1. Pre-Pruning Hyperparameters: max_depth, min_samples_split, min_samples_leaf\n",
    "2. Post-Pruning Hyperparameter: ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb42f1-8d84-4f67-81c8-dc79d268a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepruning\n",
    "decision_tree_classifier_m_ad_pre = DecisionTreeClassifier(max_depth=8, min_samples_split=2, min_samples_leaf=1, random_state=43)\n",
    "ads.custom_crossvalidation(X_ad_e_train, y_ad_train, decision_tree_classifier_m_ad_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73d989-f54d-4975-9b7f-31cad12e746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_pre.fit(X_ad_e_train, y_ad_train)\n",
    "print(decision_tree_classifier_m_ad_pre.get_depth(), decision_tree_classifier_m_ad_pre.get_n_leaves())\n",
    "plt.figure(figsize=(25, 16))\n",
    "plot_tree(decision_tree_classifier_m_ad_pre, \n",
    "          filled=True, \n",
    "          feature_names=list(X_ad_e_train.columns), \n",
    "          class_names=decision_tree_classifier_m_ad_pre.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3165084-e6af-448a-a448-e46bad4a04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can output the depth and the number of leaf nodes\n",
    "decision_tree_classifier_m_ad_pre.get_depth(), decision_tree_classifier_m_ad_pre.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35ef65-0f39-44b3-bcac-aeda8baa8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decision_tree_classifier_m_ad_pre.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b6d6f-db63-499a-bab3-8e07e8d2c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_pre.decision_path(X_ad_e_train, check_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a81b4-1c61-4b7d-86fb-52e41e615cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postpruning\n",
    "decision_tree_classifier_m_ad_post = DecisionTreeClassifier(ccp_alpha=0.005, random_state=43)\n",
    "ads.custom_crossvalidation(X_ad_e_train, y_ad_train, decision_tree_classifier_m_ad_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f3a6a-ce65-4afe-bcb3-f45e0ef8dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_post.fit(X_ad_e_train, y_ad_train)\n",
    "print(decision_tree_classifier_m_ad_post.get_depth(), decision_tree_classifier_m_ad_post.get_n_leaves())\n",
    "plt.figure(figsize=(25, 16))\n",
    "plot_tree(decision_tree_classifier_m_ad_post, \n",
    "          filled=True, \n",
    "          feature_names=list(X_ad_e_train.columns), \n",
    "          class_names=decision_tree_classifier_m_ad_post.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e58328-bfc4-4947-b573-1ad363fabcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can output the depth and the number of leaf nodes\n",
    "decision_tree_classifier_m_ad_post.get_depth(), decision_tree_classifier_m_ad_post.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572e3dd-6f1f-4195-92bf-2113975ee818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decision_tree_classifier_m_ad_post.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946f301-4438-4bc5-943b-449e22b2efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_post.decision_path(X_ad_e_train, check_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56efde-a249-4b03-92f8-7ac057b67d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre/postpruning\n",
    "decision_tree_classifier_m_ad_prepost = DecisionTreeClassifier(max_depth=8, min_samples_split=2,min_samples_leaf=1, ccp_alpha=0.005,random_state=43)\n",
    "ads.custom_crossvalidation(X_ad_e_train, y_ad_train, decision_tree_classifier_m_ad_prepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50ff89-2c2d-4d04-8b2c-ccdeb77fd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_prepost.fit(X_ad_e_train, y_ad_train)\n",
    "print(decision_tree_classifier_m_ad_prepost.get_depth(), decision_tree_classifier_m_ad_prepost.get_n_leaves())\n",
    "plt.figure(figsize=(25, 16))\n",
    "plot_tree(decision_tree_classifier_m_ad_prepost, \n",
    "          filled=True, \n",
    "          feature_names=list(X_ad_e_train.columns), \n",
    "          class_names=decision_tree_classifier_m_ad_prepost.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ea464-49b0-4186-87ad-ebd12f142ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can output the depth and the number of leaf nodes\n",
    "decision_tree_classifier_m_ad_prepost.get_depth(), decision_tree_classifier_m_ad_prepost.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0bd221-7209-4d38-900a-e68f53d77f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decision_tree_classifier_m_ad_prepost.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ddd45-de35-4cb4-a4e2-5cb35d31f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier_m_ad_prepost.decision_path(X_ad_e_train, check_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3386ef-c2ee-4551-a2b3-4ef210dcd5de",
   "metadata": {},
   "source": [
    "**Find Optimal Model using a grid search**\n",
    "\n",
    "Optimise for f-measure rather than accuracy, given the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9700aa-ae43-491b-88d6-e28c76c9f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': range(3, 9),\n",
    "    'min_impurity_decrease' : np.arange(0.01, 0.3, 0.01),\n",
    "    'min_samples_leaf': range(1, 10, 1),\n",
    "    'min_samples_split': range(2, 10, 1),\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'ccp_alpha': [0.003, 0.005, 0.007]\n",
    "}\n",
    "\n",
    "params_grid_2 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': range(3, 13),\n",
    "    #'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n",
    "    #'min_impurity_decrease' : np.arange(0.01, 0.3, 0.01),\n",
    "    #'min_samples_split': [2, 3, 4, 5, 10, 20],\n",
    "    #'min_samples_leaf': [1, 2, 3, 4, 5, 10],\n",
    "    'ccp_alpha': [0.003, 0.004, 0.005]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier_m_ad, param_grid=params_grid_2, scoring='f1_macro', cv=5, n_jobs=-1)\n",
    "#grid_search = GridSearchCV(decision_tree_classifier_m_ad, param_grid=params_grid_2, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_ad_e_train, y_ad_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d26db7-d6f6-4b50-b53f-7a8cd40d5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "XX_ad_train, XX_ad_test, yy_ad_train, yy_ad_test = train_test_split(X_ad, y_ad,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    stratify=y_ad,\n",
    "                                                                    random_state=43)\n",
    "\n",
    "best_decision_tree_classifier_m_ad_prepost = Pipeline([\n",
    "    # unknown categories are handled by mapping them to -1\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "    ('sklearn_dt', grid_search.best_estimator_)\n",
    "])\n",
    "best_decision_tree_classifier_m_ad_prepost.fit(XX_ad_train, yy_ad_train)\n",
    "\n",
    "ads.custom_crossvalidation(XX_ad_train, yy_ad_train, best_decision_tree_classifier_m_ad_prepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338fa38-bc57-4afe-8f82-f33442caaccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_ad_test_pred = best_decision_tree_classifier_m_ad_prepost.predict(XX_ad_test)\n",
    "print(classification_report(yy_ad_test, yy_ad_test_pred))\n",
    "ads.plot_confusion_matrix(yy_ad_test, yy_ad_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdabe03-8056-4756-9849-acf489a84fe5",
   "metadata": {},
   "source": [
    "**XGboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd10dbe-644e-4d6b-8244-4447ea871345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "XX_ad = X_ad.copy()\n",
    "XX_ad_c = XX_ad.select_dtypes(include='object').columns\n",
    "XX_ad[XX_ad_c] = XX_ad[XX_ad_c].astype('category')\n",
    "XX_ad.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a745449-660f-44d0-a9f9-8a14962e6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_ad = y_ad.copy()\n",
    "yy_ad_e = yy_ad['label'].map({'<=50K': 0, '>50K': 1})\n",
    "yy_ad_e.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66c225-a70f-486d-b976-76c47b7499d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_ad_e.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bcd16d-f459-4cde-827b-60ecd4db5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "XX_ad_train, XX_ad_test, yy_ad_e_train, yy_ad_e_test = train_test_split(XX_ad, yy_ad_e, test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710a3e9-2b17-4285-8547-78ed785462cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a single decision tree; leave other hyperparameters to default values\n",
    "xgb_dt_m_ad = XGBClassifier(enable_categorical=True, \n",
    "                            n_estimators=1, \n",
    "                            num_parallel_tree=1, \n",
    "                            objective='multi:softmax', \n",
    "                            booster='dart',\n",
    "                            num_class=2, \n",
    "                            eval_metric='mlogloss',\n",
    "                            random_state=43)\n",
    "# crossvalidate and output performance\n",
    "ads.custom_crossvalidation(XX_ad_train, yy_ad_e_train, xgb_dt_m_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7e240-9ffb-48ff-910f-65a884858bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dt_m_ad.fit(XX_ad_train, yy_ad_e_train)\n",
    "xgb_dt_m_ad_tree_text = xgb_dt_m_ad.get_booster().get_dump(with_stats=True)\n",
    "# Print the decision rules for the first tree\n",
    "print(xgb_dt_m_ad_tree_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8c058-cf15-4e72-a360-cf6d17930355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import graphviz\n",
    "fig = plt.figure(figsize=(25, 16))\n",
    "ax = fig.gca()\n",
    "# num_trees specifies the index of the tree to plot\n",
    "xgb.plot_tree(xgb_dt_m_ad, num_trees=0, rankdir='LR', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c752b-018d-42fd-aea4-aa0de58e95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 40))\n",
    "ax = fig.gca()\n",
    "# num_trees specifies the index of the tree to plot\n",
    "xgb.plot_tree(xgb_dt_m_ad, num_trees=1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b645151-6f72-48d8-a923-a3881919c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "params = {\n",
    "    'max_depth' : range(3, 11),\n",
    "    #'n_estimators': [1, 3, 5, 7],\n",
    "    #'num_parallel_tree': [1, 3, 5],  # Number of trees to build in parallel per boosting round\n",
    "    'max_leaves':[3, 5, 7, 10],\n",
    "    'learning_rate': np.arange(0.1, 0.5, 0.1),\n",
    "    'min_child_weight': range(1,6,2)\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "    #'n_estimators': [100, 200, 300, 400],\n",
    "    'n_estimators': [21],\n",
    "    'num_parallel_tree': [1],\n",
    "    'learning_rate': uniform(0.01, 0.5),\n",
    "    'max_depth' : range(3, 13),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    #'max_leaves':[3, 5, 7, 10],\n",
    "    'min_child_weight': range(1,6,1)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb_dt_m_ad, \n",
    "                                   param_distributions=params2, \n",
    "                                   n_iter=200, \n",
    "                                   cv=5, \n",
    "                                   n_jobs=-1, \n",
    "                                   random_state=43, \n",
    "                                   scoring='f1_macro')\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search.fit(XX_ad_train, yy_ad_e_train)\n",
    "print(random_search.best_params_)\n",
    "\n",
    "\n",
    "# optimise for f1-measure\n",
    "# grid_search = GridSearchCV(xgb_dt_m_ad, param_grid=params3, scoring='f1_macro', cv=5)\n",
    "# grid_search.fit(XX_ad_train, yy_ad_e_train)\n",
    "# print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e67039-90e0-4563-9247-28dc4707e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(np.mean(grid_search.cv_results_['mean_test_score']),\n",
    "#np.std(grid_search.cv_results_['mean_test_score']))\n",
    "(np.mean(random_search.cv_results_['mean_test_score']),\n",
    "np.std(random_search.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d3040-7a2e-4a70-b8c7-5e13e509530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_xgb = random_search.best_estimator_\n",
    "ads.custom_crossvalidation(XX_ad_train, yy_ad_e_train, optimal_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a6e5d-0dbd-4616-8a4d-21fadc59e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_ad_e_test_pred = optimal_xgb.predict(XX_ad_test)\n",
    "print(classification_report(yy_ad_e_test, yy_ad_e_test_pred))\n",
    "ads.plot_confusion_matrix(yy_ad_e_test, yy_ad_e_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c242f45-d9e3-4372-ab2b-8473cfaa7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 16))\n",
    "ax = fig.gca()\n",
    "# num_trees specifies the index of the tree to plot\n",
    "xgb.plot_tree(optimal_xgb, num_trees=1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve, auc\n",
    "\n",
    "# ROC curves for 3 optimal models\n",
    "best_lr_m_ad_disp = RocCurveDisplay.from_estimator(best_logistic_regression_m_ad, X_ad_e_s_test, y_ad_e_s_test)\n",
    "\n",
    "best_dtc_m_ad_prepost_named_disp = RocCurveDisplay.from_estimator(\n",
    "    best_decision_tree_classifier_m_ad_prepost,\n",
    "    XX_ad_test, yy_ad_test,\n",
    "    ax=best_lr_m_ad_disp.ax_\n",
    ")\n",
    "\n",
    "best_xgb_m_dt_disp = RocCurveDisplay.from_estimator(\n",
    "    optimal_xgb,\n",
    "    XX_ad_test, yy_ad_e_test,\n",
    "    ax=best_dtc_m_ad_prepost_named_disp.ax_\n",
    ")\n",
    "\n",
    "best_xgb_m_dt_disp.figure_.suptitle(\"ROC curve comparison\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
