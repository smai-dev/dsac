{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31ab7d1-36c6-4fc5-b17a-ee1140151fd1",
   "metadata": {},
   "source": [
    "# **Clustering**\n",
    "\n",
    "The clustering dataset consists of generated data, which means there is no specific business context or attribute descriptions, as mentioned in the assessment documents. The necessary Python libraries and their API (Application Programming Interface) functions are imported at the relevant sections of the document, rather than importing them all at the beginning, ensuring clarity and improving the document's organization.\n",
    "\n",
    "The report is divided into the following main sections:\n",
    "- The nature of the generated data attributes is explained, and additional details about key concepts are provided for better understanding.\n",
    "- Insights on the data are presented through relevant plots, and initial observations about the clusters are included.\n",
    "- The data is prepared for use with the clustering algorithms, with details about the scaling and normalization processes included.\n",
    "- Two algorithms are chosen for clustering the dataset, and the reasons for their selection are justified.\n",
    "- Analyses are provided on how to determine if the clustering results are correct in the absence of class labels, while following the guidelines outlined in the assessment material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5113fa8-fc06-45b9-90b1-8f77677226d1",
   "metadata": {},
   "source": [
    "## **1. Import, explore and visualize the data to gain insights**\n",
    "\n",
    "The data is first loaded into a `DataFrame` data structure from the `assessment_cluster_dataset.csv` file using the *Pandas* library. Pandas is useful for working with table data like in spreadsheets or databases. It helps to explore, clean, and process the data. In Pandas, a data table is stored in a `DataFrame`. The *Numpy* library is also imported because it can be used by Pandas for data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8530d8-6fa9-4d8b-a78d-2ac71ec24de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line sets the filter for warnings. It tells Python to ignore all warnings that are generated during the execution of the program.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import numpy and pandas libraries to import the data set into a Dataframe\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Load the data in DataFrame\n",
    "data = pd.read_csv('data/dataset.csv', sep=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942f298-79ad-426d-b583-1672036bf653",
   "metadata": {},
   "source": [
    "### **Exploring data information**\n",
    "\n",
    "The output information about the dataset, provided by `data.info()`, includes the number of rows, the number of columns, the names (data attributes) of the columns, how many entries in each column are not missing, and the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8f7a5-3be4-496d-ba26-0f2d87027196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb26695-37de-42e9-971c-60b5a888c69a",
   "metadata": {},
   "source": [
    "The data imported into the DataFrame contains **1500** entries, numbered from **0** to **1499**. There are **3 columns** corresponding to **3 attributes** in total (**att1**, **att2**, and **att3**). Each column has **1500 non-null values** with the type `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d359991-89ea-429d-9169-dcd2c2fe0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5083e5-c8d4-4f07-92f7-6ca45dc1f635",
   "metadata": {},
   "source": [
    "The information displayed by `data.describe()` are the following:\n",
    "- **count**: the number of non-null entries in a column used for the statistics.\n",
    "- **mean**:  the **average of the values** in a column (for numbers only).\n",
    "- **std**:   the **standard deviation** in a column (for numbers only).\n",
    "- **min**:   the smallest value in a column (for numbers only).\n",
    "- **25%**:   the **first quartile** or **25th percentile** in a column (for numbers only).\n",
    "- **50%**:   the **second quartile** or **50th percentile/median** in a column (for numbers only).\n",
    "- **75%**:   the **third quartile** or **75th percentile** in a column (for numbers only).\n",
    "- **max**:   the largest value in a column (for numbers only).\n",
    "\n",
    "#### **Standard diviation**\n",
    "\n",
    "\n",
    "The **standard deviation** of a column in a dataset measures **how much the values in that column differ from the average (or mean) value**. The standard deviation shows how spread out the values are around the mean, where a **low standard deviation** means the values are **close to the mean with less difference**, and a **high standard deviation** means the values are **more spread out with more difference**.\n",
    " \n",
    "For a dataset column $x$ with values $x_1, x_2, ..., x_n$ and mean $\\bar{x}$, the standard deviation $\\sigma_x$ is given by the following formula:\n",
    "$$\n",
    "\\text{Standard deviation (} \\sigma_x \\text{)} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "#### **Quartiles and Percentiles**\n",
    "\n",
    "A **percentile** measures **how values are spread in a column** of the dataset. It shows the value below which a certain percentage of entries fall. Quartiles split the column into four equal parts. For example, the **second quartile** (also known as the 50th percentile or median) is the **value at which 50% of the data points fall below**. This helps to understand how the data is distributed, showing the lower, middle, and upper parts of the values, as well as whether most scores are close to the mean or vary widely.\n",
    "\n",
    "### **Visualizing the data for insights**\n",
    "\n",
    "The data will now be visualized to **help understand its patterns, clusters and trends**. By using different types of 2D and 3D plots, important insights can be gained about the data. Some of these visualazations are useful to see how the values relate to each other within distinct clusters to the naked eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f7961-d16f-4fdd-999e-a2b690e448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output histograms \n",
    "data.hist(figsize=(16, 4), layout=(1, len(data.columns)), edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a57d0-e70f-4ac9-9236-266915f0b684",
   "metadata": {},
   "source": [
    "While these **histograms are not effective for identifying clusters**, as they display each attribute's data independently, they **clearly show how the data is spread around the mean value**, helping us understand the overall distribution and variation in the data values for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868b8b5-50c5-4f09-bfc9-c94f90086a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot, seaborn and itertools libraries to display 2D scatter plots\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "# Create 2D scatter plots for all possible paired combinations of the attributed\n",
    "cols = data.columns\n",
    "indices_combinations = list(combinations(range(len(cols)), 2))\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i, pair in enumerate(indices_combinations):\n",
    "    plt.subplot(1, len(indices_combinations), i + 1)  # One row, many columns\n",
    "    sns.scatterplot(data, x=cols[pair[0]], y=cols[pair[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e809bcf-aece-4748-98ac-cb4d6bd033a2",
   "metadata": {},
   "source": [
    "The 2D scatter plots offer valuable insights about the number of clusters present in the data. While they help visualize how the data points group together, accurately estimating the exact number of clusters can still be non trivial. These plots can **show patterns and relationships between attributes**, making it easier to see how they connect with one another. Overall, these 2D plots are useful for **understanding paired data correlations**, even if they **don't provide an accurate visualization** of the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1a903-158e-419d-a1ab-a10aa283c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotly.express for 3D scatter plots\n",
    "import plotly.express as px\n",
    "\n",
    "# Generate a 3d scaterplot and reduce the size of dots\n",
    "data_3d = px.scatter_3d(data, x='att1', y='att2', z='att3')\n",
    "data_3d.update_traces(marker=dict(symbol=\"circle\", size=2)) \n",
    "\n",
    "# Adjust the plot size for improved rotated visualization\n",
    "data_3d.update_layout(width=1000, height=800)\n",
    "data_3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bb851",
   "metadata": {},
   "source": [
    "![First clusters visualisation](./img/3d_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c7afc-cb9e-4d6e-b32b-d03962d1b2ed",
   "metadata": {},
   "source": [
    "The 3D scatter plots provide **clear insights into the number of clusters** and how the three attributes are connected with each other. They make it easier to see the relationships and patterns among the data points in three dimensions.\n",
    "\n",
    "These are the main characteristics:  \n",
    "- There are **7 distinct globular clusters** of different sizes.  \n",
    "- Most clusters are **dense in the center** and **more sparse at the edges**.  \n",
    "- A **few isolated scattered dots** appear to be noise in the data.  \n",
    "- Some clusters are **close to one another**, while others **are well separated**.\n",
    "\n",
    "## **2. Data preparation for clustering algorithms**\n",
    "\n",
    "One key transformation for the data is **feature scaling**. Most machine learning algorithms don't perform well when input numerical **attributes have very different scales**. Two common methods are commonly used for attribute scaling: **min-max scaling** and **standardization**.\n",
    "\n",
    "**Min-max scaling, often called normalization**, is straightforward: each attribute's values are adjusted to range from 0 to 1 by subtracting the minimum and dividing by the range (the difference between the maximum and the minimum values). Standardization works differently. It first subtracts the mean from each value (resulting in a zero mean) and then divides by the standard deviation, giving a standard deviation of 1. **Unlike min-max scaling, standardization doesntt limit values to a set range, making it less sensitive to outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329ab2c-c683-493b-b93d-fe26fe137dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler class from the sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# create the standard scaler object\n",
    "scaler = StandardScaler()\n",
    "# train the scaler on data\n",
    "scaler.fit(data)\n",
    "# apply it to data to transform it and assign it to another variable\n",
    "data_transformed = scaler.transform(data)\n",
    "# create a new dataframe\n",
    "data_scaled = pd.DataFrame(data_transformed, columns=data.columns)\n",
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d9ea3-07e0-4cea-9a0d-05c186ffd298",
   "metadata": {},
   "source": [
    "## **3. Data clustering**\n",
    "\n",
    "In this section, the selected algorithms, K-Means and DBScan, are presented and applied to the scaled data. The agglomerative hierarchical clustering was discarded, and the reasons for this decision will be provided later.\n",
    "\n",
    "### **K-Means clustering**\n",
    "\n",
    "K-means is a popular clustering method that starts by selecting $k$ random rows as initial cluster centers. Each object is assigned to the nearest cluster based on distance, and new centers are calculated for each cluster. This process **repeats** until **no objects change clusters** or **the distances to the centers stabilize**.\n",
    "\n",
    "K-means performs effectively when natural **clusters are compact and clearly separated**, as is almost the case here, and it is **efficient for high-dimensional data**. However, it does not work well when the chosen **$k$ does not match the actual number of clusters**, is **sensitive to noise and outliers**, and **performs poorly with non-globular or differently-sized clusters**. \n",
    "\n",
    "For the data studied here, there is **strong confidence in the number (7) of clusters**, and most of them are **fairly elliptically compact** and **well-separated**, except for a few. Therefore, **K-Means** is deemed a suitable algorithm for this case.\n",
    "\n",
    "In the following, two initialization techniques for K-Means, namely **`random`** and **`k-means++`**, will be applied, and their results will be presented and compared.\n",
    "\n",
    "#### **Random Initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef51e5-8303-4fbe-8d22-0f63497ee5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans class from sklearn.cluster library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# create the kmeans clustering object\n",
    "kmeans_random_model = KMeans(n_clusters=7, init='random').set_output(transform='pandas')\n",
    "# kmeans_random_model = KMeans(n_clusters=7, init='random', random_state=42).set_output(transform='pandas')\n",
    "\n",
    "# train and transform the scaled data\n",
    "results_kmeans_random = kmeans_random_model.fit_transform(data_scaled)\n",
    "results_kmeans_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e682289-64cc-498e-9906-10fb4fd9996b",
   "metadata": {},
   "source": [
    "The important hyperparameter to explain is **`random_state`**. It sets a random number for starting centroids, which makes results consistent when using the same number. **Using an integer for this helps get the same results every time**, but **<font color='red'>it's good to check if results stay stable with different random seeds</font>**. Common choices for these seeds are **0** and **42**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa6fcd-371a-4165-af94-eba065bf146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f04594-c1e6-4c11-946b-ea9688e0c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4c2eb-518d-4580-b711-05bb9f5c189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_data = data_scaled.copy()\n",
    "kmeans_random_data['cluster'] = kmeans_random_model.labels_\n",
    "kmeans_random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d477d3-ffc6-480a-879e-046b0efcfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_centroids = pd.DataFrame(kmeans_random_model.cluster_centers_, columns=data_scaled.columns)\n",
    "kmeans_random_centroids['cluster'] = ['Centroid 0','Centroid 1','Centroid 2','Centroid 3','Centroid 4','Centroid 5','Centroid 6']\n",
    "kmeans_random_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd711df1-e3c1-44d3-93a5-73f553911b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import parallel_coordinates plotting class from pandas.plotting library\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Plot the centroids across all attributes\n",
    "parallel_coordinates(kmeans_random_centroids, 'cluster',  marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858250f-a1f7-4d75-b20f-2825ae1eb3a9",
   "metadata": {},
   "source": [
    "After training the model several times, **it was observed that all the centroids are stable, different and do not overlap**. This visualization of centroids helps to illustrate how each cluster is formed and how they are positioned in relation to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb5725-a657-47c6-b4c4-876e904840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e11a3c-52ee-4ec0-8360-3a8533f712bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for data points\n",
    "fig_3d_kmeans_random_data = px.scatter_3d(kmeans_random_data, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_random_data.update_traces(marker=dict(symbol=\"circle\", size=2), opacity=0.7, name=\"Data\")\n",
    "\n",
    "# Plot for centroids\n",
    "fig_3d_kmeans_random_centroids = px.scatter_3d(kmeans_random_centroids, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_random_centroids.update_traces(marker=dict(symbol=\"x\", size=5), name=\"Centroids\")\n",
    "\n",
    "# Combine centroids into the main figure with adjusted legend entries\n",
    "for i, trace in enumerate(fig_3d_kmeans_random_centroids.data):\n",
    "    cluster_value = kmeans_random_centroids['cluster'].iloc[i]\n",
    "    trace.name = f\"{cluster_value}\"\n",
    "    fig_3d_kmeans_random_data.add_trace(trace)\n",
    "\n",
    "# Update layout to separate legends, position the first legend on the left and resize the final 3D plot\n",
    "fig_3d_kmeans_random_data.update_layout(legend=dict(title=\"Centroids\", x=0, xanchor=\"left\",y=1), width=1000, height=800, \n",
    "                                        title=\"D Scatter Plot with Centroids (k=7, init=random, default for other hyperparameters)\")\n",
    "\n",
    "# Add a second legend for centroids (workaround using annotations for the second legend)\n",
    "fig_3d_kmeans_random_data.add_layout_image(dict(source=None,\n",
    "                                                # Invisible image to create spacing for a legend\n",
    "                                                x=1.1, y=1, xanchor=\"right\", yanchor=\"top\", layer=\"above\"))\n",
    "\n",
    "# Adjust layout to position centroids legend\n",
    "fig_3d_kmeans_random_data.update_traces(selector=dict(name=\"Centroid\"),showlegend=True)\n",
    "fig_3d_kmeans_random_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fc570",
   "metadata": {},
   "source": [
    "![3D Scatter Plot with Centroids (k=7, init=random, default for other hyperparameters)](./img/3d_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44777c3d-f8ab-43de-94b6-fee068948d93",
   "metadata": {},
   "source": [
    "According to the 3D plotting, it is clear that the **seven clusters are well identified** and **the centroids are well positioned in the center of the different clusters**. This indicates that the random initialization of the K-Means clustering is **effective and demonstrates a high level of stability**.\n",
    "\n",
    "Let's consider two randomly scaled rows to predict their clusters using the trained K-Means model with random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5b2e8-6f76-4805-afd1-f5740e490c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = [[0.5, -1.6, -0.2], [-1.5, 1, 0.4]]\n",
    "kmeans_random_model.predict(new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39a6b4-b0eb-4328-bc3c-ac2a8229e7b6",
   "metadata": {},
   "source": [
    "#### **K-mean++ Initialisation**\n",
    "\n",
    "K-means++ improves the starting position of cluster centers by making them far apart. First, it picks the first cluster center randomly. Then, for each remaining center, the algorithm weights all rows based on how close they are to the nearest cluster center, making rows that are far away more likely to be chosen as the next center.\n",
    "\n",
    "The **results of the `k-mean++` and `random` inisizalization methods are almost similar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0196ab2-f3f5-4771-bde4-0e4261a0a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model = KMeans(n_clusters=7, init='k-means++').set_output(transform='pandas')\n",
    "results_kmeans_pp = kmeans_pp_model.fit_transform(data_scaled)\n",
    "results_kmeans_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962706a-91d4-4e75-ab64-dd5c3c0d0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c77e6-dfc4-4504-9a39-1b42e39c5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31ac70-d1a0-46c8-8446-3abb591da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_data = data_scaled.copy()\n",
    "kmeans_pp_data['cluster'] = kmeans_pp_model.labels_\n",
    "kmeans_pp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097c558-460b-4d29-a1a2-f45a67aa27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_centroids = pd.DataFrame(kmeans_pp_model.cluster_centers_, columns=data_scaled.columns)\n",
    "kmeans_pp_centroids['cluster'] = ['Centroid 0','Centroid 1','Centroid 2','Centroid 3','Centroid 4','Centroid 5','Centroid 6']\n",
    "kmeans_pp_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414ec77-2190-4257-adbf-aa2ea0152a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_coordinates(kmeans_pp_centroids, 'cluster',  marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755a61d-656f-4dd3-9cfa-1e130fd5ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_random_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3efda-0203-4feb-b52c-6af8e8e93397",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d2e8b-f89d-4c1c-8e5b-17fec90e2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for data points\n",
    "fig_3d_kmeans_pp_data = px.scatter_3d(kmeans_pp_data, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_pp_data.update_traces(marker=dict(symbol=\"circle\", size=2), opacity=0.6, name=\"Data\")\n",
    "\n",
    "# Plot for centroids\n",
    "fig_3d_kmeans_pp_centroids = px.scatter_3d(kmeans_pp_centroids, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_pp_centroids.update_traces(marker=dict(symbol=\"x\", size=5), name=\"Centroids\")\n",
    "\n",
    "# Combine centroids into the main figure with adjusted legend entries\n",
    "for i, trace in enumerate(fig_3d_kmeans_pp_centroids.data):\n",
    "    cluster_value = kmeans_pp_centroids['cluster'].iloc[i]\n",
    "    trace.name = f\"{cluster_value}\"\n",
    "    fig_3d_kmeans_pp_data.add_trace(trace)\n",
    "\n",
    "# Update layout to separate legends\n",
    "fig_3d_kmeans_pp_data.update_layout(legend=dict(title=\"Data\", x=0, xanchor=\"left\", y=1), width=1000, height=800,\n",
    "                                    title=\"3D Scatter Plot with Centroids (k=7, init=k-means++, default for other hyperparameters)\")\n",
    "\n",
    "# Add a second legend for centroids (workaround using annotations for the second legend)\n",
    "fig_3d_kmeans_pp_data.add_layout_image(dict(source=None, x=1.1, y=1, xanchor=\"right\", yanchor=\"top\", layer=\"above\"))\n",
    "\n",
    "# Adjust layout to position centroids legend\n",
    "fig_3d_kmeans_pp_data.update_traces(selector=dict(name=\"Centroid\"),showlegend=True)\n",
    "fig_3d_kmeans_pp_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1716841",
   "metadata": {},
   "source": [
    "![3D Scatter Plot with Centroids (k=7, init=k-means++, default for other hyperparameters)](./img/3d_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09b522-4aa1-47cb-b4ac-132d1885df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model.predict(new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ffd9d-e1b2-437f-9ffd-e949a838196d",
   "metadata": {},
   "source": [
    "#### **Searching for the optimal configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c616740-799b-4d11-92bb-1e8a22613c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the range of values to try out for each hyperparameter\n",
    "param_grid = {\n",
    "    'n_clusters': range(6, 9),\n",
    "    'init': ['k-means++', 'random'],\n",
    "    'n_init': [5, 10, 15],\n",
    "    'max_iter': [100, 200, 300, 400, 500],\n",
    "    'random_state': [1, 16, 34, 42, 57]\n",
    "}\n",
    "# Create the kmeans model\n",
    "kmeans = KMeans()\n",
    "# Use the grid search to try out all possible combinations of hyperparameter values from the grid above and fit a new model for each\n",
    "grid_search = GridSearchCV(kmeans, param_grid)\n",
    "grid_search.fit(data_scaled)\n",
    "# output the hyperparameter values of the model that achieved highest performance\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26feac26-aa5a-4d1a-a219-f20c29db84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model = KMeans(n_clusters=8, init='k-means++', \n",
    "                         n_init=5, max_iter=100, random_state=42).set_output(transform='pandas')\n",
    "results_kmeans_pp = kmeans_pp_model.fit_transform(data_scaled)\n",
    "results_kmeans_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99ef5c-972a-40e5-b75d-c90fd871bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c5519-21c5-4f49-a459-e122479d293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce92804-f217-4fdc-9427-c85daf4ace35",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_data = data_scaled.copy()\n",
    "kmeans_pp_data['cluster'] = kmeans_pp_model.labels_\n",
    "kmeans_pp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ddcb3-3030-48ef-96fa-5539fa6579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_centroids = pd.DataFrame(kmeans_pp_model.cluster_centers_, columns=data_scaled.columns)\n",
    "kmeans_pp_centroids['cluster'] = ['centroid 0','centroid 1','centroid 2','centroid 3','centroid 4','centroid 5','centroid 6','centroid 7']\n",
    "kmeans_pp_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cec0e-7d56-4d72-8a5a-a72b93030d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_coordinates(kmeans_pp_centroids, 'cluster',  marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae984993-25c9-4ff0-880f-cbe62090221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pp_data['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07027219-f835-4961-931e-e294099d1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for data points\n",
    "fig_3d_kmeans_pp_data = px.scatter_3d(kmeans_pp_data, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_pp_data.update_traces(marker=dict(symbol=\"circle\", size=2), opacity=0.6, name=\"Data\")\n",
    "\n",
    "# Plot for centroids\n",
    "fig_3d_kmeans_pp_centroids = px.scatter_3d(kmeans_pp_centroids, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_kmeans_pp_centroids.update_traces(marker=dict(symbol=\"x\", size=5), name=\"Centroids\")\n",
    "\n",
    "# Combine centroids into the main figure with adjusted legend entries\n",
    "for i, trace in enumerate(fig_3d_kmeans_pp_centroids.data):\n",
    "    cluster_value = kmeans_pp_centroids['cluster'].iloc[i]\n",
    "    trace.name = f\"{cluster_value}\"\n",
    "    fig_3d_kmeans_pp_data.add_trace(trace)\n",
    "\n",
    "# Update layout to separate legends\n",
    "fig_3d_kmeans_pp_data.update_layout(legend=dict(title=\"Data\", x=0, xanchor=\"left\", y=1), width=1000, height=800, \n",
    "                                    title=\"3D Scatter plot with centroids (GridSearch optimal hyperparameters)\")  \n",
    "\n",
    "# Add a second legend for centroids (workaround using annotations for the second legend)\n",
    "fig_3d_kmeans_pp_data.add_layout_image(dict(source=None,# Invisible image to create spacing for a legend\n",
    "                                            x=1.1, y=1, xanchor=\"right\", yanchor=\"top\", layer=\"above\"))\n",
    "\n",
    "# Adjust layout to position centroids legend\n",
    "fig_3d_kmeans_pp_data.update_traces(selector=dict(name=\"Centroid\"),showlegend=True)\n",
    "fig_3d_kmeans_pp_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3565c6",
   "metadata": {},
   "source": [
    "![3D Scatter plot with centroids (GridSearch optimal hyperparameters)](./img/3d_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b926ad-d4fb-440b-9ecf-855510916f82",
   "metadata": {},
   "source": [
    "In Figures 1 and 2, the lower Cluster 1 (in the 7-cluster setup) is spread out elliptically across `att2` but splits into two clusters in the optimal configuration found by GridSearch (8 clusters). Seven clusters may actually be more accurate. This split in the optimal configuration could be due to the following:\n",
    "\n",
    "1. **Globular clouds**: K-Means works well when clusters are compact, well-separated clouds.\n",
    "2. **Noise sensitivity**: K-Means is sensitive to noise and outliers, which are present here.\n",
    "3. **Cluster shape and size**: K-Means performs poorly when clusters are not distinct or vary in size, especially around sparse Cluster 1 and its neighboring clusters.\n",
    "\n",
    "**Objective evaluation** of the K-Means clustering will help confirm our subjective decision regarding the correct number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de542c-5336-46fd-b593-85f64b660ef9",
   "metadata": {},
   "source": [
    "#### **K-Means clustering evaluation**\n",
    "\n",
    "Subjective analyses were already provided gradually in the previous content. This paragraph will be dedicated for the objective analysis of the K-Means clustering with random initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f0435-e3ba-462c-9622-b09355f164c9",
   "metadata": {},
   "source": [
    "##### **Deciding the best number of clusters for K-Means**\n",
    "\n",
    "###### **Elbow method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95912933-dcc3-477b-8879-c9e60da21aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(2, 11):\n",
    "    # Only the n_clusters hyperparameter will vary, while the others will remain fixed according to the GridSearch results.    \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=5, max_iter=100, random_state=42)\n",
    "    kmeans.fit(data_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "plt.plot(range(2, 11), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782bc7a-5ac5-442d-8c16-9e4420891e49",
   "metadata": {},
   "source": [
    "###### **Davies-Bouldin and Silhouette indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af169ef1-2434-469e-bbfe-dc5dedc55f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "db_scores = []\n",
    "sil_scores = []\n",
    "for k in range(2, 11):\n",
    "  kmeans = KMeans(n_clusters=k, init='k-means++', n_init=5, max_iter=100, random_state=42)\n",
    "  labels = kmeans.fit_predict(data_scaled)\n",
    "  db_scores.append(davies_bouldin_score(data_scaled, labels))\n",
    "  sil_scores.append(silhouette_score(data_scaled, labels))\n",
    "plt.subplots()\n",
    "plt.plot(range(2, 11), db_scores, marker='o')\n",
    "plt.plot(range(2, 11), sil_scores, marker='x')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(['Davies-Bouldin', 'Silhouette'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0666af-67df-40c4-9d06-d2c7151332ef",
   "metadata": {},
   "source": [
    "Given that the Elbow method diagram **doesn't reflect accuretly the number of clusters**, as the convergence starts at 7 clusters, it doesn't help to **decide about the right number of clusters**. However, the Davies-Bouldin (DBI) and Slihouette (SI) curves provide us with clear accurate indications:\n",
    "1. The higher **SI value 7 indicates the best clustering schema and confirms the subjective observations**.\n",
    "2. The intersections after the lower DBI values and the higher SI value occur between 7 and 8 clusters, confirming the tradeoff between these two options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca764e-fa7e-48d5-b180-b62c64b58d1f",
   "metadata": {},
   "source": [
    "### **DBScan (density-based) clustering**\n",
    "\n",
    "DBSCAN was selected as the second algorithm along with K-Means because it has some special advantages. Unlike K-Means, which works best with round clusters of similar sizes, **DBSCAN can find clusters of different shapes and sizes**. This means it can discover clusters that K-Means might miss. Also, DBSCAN is **good at ignoring noise points and border points**, so it **works well with data that has outliers**. However, DBSCAN can have trouble with clusters that have different densities since it uses fixed values for the `eps` and `min_samples` parameters. **Overall, DBSCAN's capability to manage complex diverse clusters with noise makes it useful choice alongside K-Means, which is a perfect fit for our situation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50eb5d-fff3-4eca-ae97-199221846823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan_model = DBSCAN()\n",
    "dbscan_labels = dbscan_model.fit_predict(data_scaled)\n",
    "dbscan_data = data_scaled.copy()\n",
    "dbscan_data['cluster'] = dbscan_model.labels_\n",
    "dbscan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e251bd-12de-4346-aec8-3789e76ca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_data.groupby('cluster')['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d898c56-ac84-4977-b2c3-0e893ce1f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d_dbscan_data = px.scatter_3d(dbscan_data, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_dbscan_data.update_traces(marker=dict(symbol=\"circle\", size=2), name=\"Data\")\n",
    "fig_3d_dbscan_data.update_layout(width=1000, height=800,\n",
    "                                 title=\"3D Scatter plot for DBScan (default hyperparameters)\")\n",
    "fig_3d_dbscan_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc92efd",
   "metadata": {},
   "source": [
    "![3D Scatter plot for DBScan (default hyperparameters)](./img/3d_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b2a54-ec39-4a26-9188-acf966e1517c",
   "metadata": {},
   "source": [
    "Using the default hyperparameters (`min_samples=5` and `eps=0.5`) gave poor results, identifying only 2 noise points (cluster -1) and 3 clusters, which is much less than the expected 7 clusters. This is mainly because the `min_samples` value is not set correctly; it should be at least the dimensionality of the dataset plus 1 (3 + 1). Additionally, the default `eps=0.5` led to too few noise points, while the actual data has around hundreds of noise points, as can be seen visually. The other hyperparameters are better left at their default values.\n",
    "\n",
    "Thatâ€™s why we need to find the correct values for `eps` and `min_samples` to achieve at least 7 clusters, along with an accurate count of noise and border points. To achieve this, we need to create a k-distance plot following the instructions given in the lab sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4d050-5bd2-4ab2-bfbb-9eb6a7305d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plot_k_distance(k):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(data_scaled)\n",
    "    # Get distances to the k-th nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(data_scaled)\n",
    "    # Sort the distances to the k-th neighbor\n",
    "    distances = np.sort(distances[:, k - 1])  # Adjust index for zero-based indexing\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances, marker='o', markersize=2.5)\n",
    "    plt.ylabel('k-distance')\n",
    "    plt.xlabel('Points sorted by distance to k-th nearest neighbor')\n",
    "    plt.title(f'k-Distance Plot for k={k}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "k = 10\n",
    "plot_k_distance(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd308ce0-eea3-4802-bdd1-28029b90751d",
   "metadata": {},
   "source": [
    "We can now empirically evaluate different combinations of `min_samples` (in the range [4, 10]) and `eps` (in the range\n",
    "from 0.2 to 1 exclusively, using a step of 0.01) using a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828dabb6-b207-48f3-9876-c2129c72b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "param_grid = {\n",
    "    'eps': np.arange(0.2, 1, 0.01),\n",
    "    'min_samples': range(4, 10)\n",
    "}\n",
    "dbscan = DBSCAN()\n",
    "grid_search = GridSearchCV(dbscan, param_grid, scoring=silhouette_score)\n",
    "grid_search.fit(data_scaled)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921df28-5e9f-4654-958c-e5d6b155bdf7",
   "metadata": {},
   "source": [
    "The results indicate that the optimal values are `eps=0.2` and `min_samples=4`. However, to achieve the correct number of noise points, an `eps` value of `2.3` yields the best outcome of 7 clusters while accurately identifying a mostly perfect cloud of noise and border points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b84f3-57d5-47a5-945d-b118a08364d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps=0.23, min_samples=4)\n",
    "dbscan_labels = dbscan_model.fit_predict(data_scaled)\n",
    "dbscan_data = data_scaled.copy()\n",
    "dbscan_data['cluster'] = dbscan_model.labels_\n",
    "dbscan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03b406-a3ee-4ca5-9775-ebf2bd0fdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_data.groupby('cluster')['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc0683-4dcc-45c3-87d3-74cee4d38941",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_3d_dbscan_data = px.scatter_3d(dbscan_data, x='att1', y='att2', z='att3', color='cluster')\n",
    "fig_3d_dbscan_data.update_traces(marker=dict(symbol=\"circle\", size=2), name=\"Data\")\n",
    "fig_3d_dbscan_data.update_layout(width=1000, height=800, title=\"3D Scatter plot for DBScan (with the optimal hyperparameters eps=0.23 and min_samples=4)\")\n",
    "fig_3d_dbscan_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2f704",
   "metadata": {},
   "source": [
    "![3D Scatter plot for DBScan (with the optimal hyperparameters eps=0.23 and min_samples=4)](./img/3d_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fb802",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
