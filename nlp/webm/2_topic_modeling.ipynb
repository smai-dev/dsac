{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import make_scorer, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # Required for 3D projection\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.data.path.append(\"./data\")\n",
    "nltk.download('punkt', download_dir=\"./data\")\n",
    "nltk.download(\"punkt_tab\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=\"./data\")\n",
    "nltk.download('stopwords', download_dir=\"./data\")\n",
    "nltk.download('wordnet', download_dir=\"./data\")\n",
    "\n",
    "import text_mining_utils as tmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data (using Step 0 logic) ---\n",
    "# Load Data ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv(\"./data/spunout_data.csv\", encoding='utf-8-sig')\n",
    "\n",
    "    # Ensure titles are strings and handle potential NaN values by replacing them with empty strings\n",
    "    df['Title'] = df['Title'].fillna('').astype(str)\n",
    "\n",
    "    # Remove rows where content extraction failed (marked as 'N/A' by the scraper)\n",
    "    # Also drop rows where Content is actually NaN\n",
    "    df = df[df['Content'] != 'N/A']\n",
    "    df = df.dropna(subset=['Content'])\n",
    "\n",
    "    # Concatenate the Title with the Content\n",
    "    # We add a space in between to prevent the last word of the title merging with the first word of the body\n",
    "    df['Content'] = df['Title'] + \" \" + df['Content']\n",
    "\n",
    "    print(f\"Successfully loaded {len(df)} articles.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at './data/spunout_data.csv'. Please run the scraper first.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7beda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Standard English Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Your Domain-Specific Stopwords (Crucial for filtering \"Common Words\")\n",
    "domain_stopwords = [\n",
    "    # Site & Web specific\n",
    "    'spunout', 'spun', 'out', 'ie', 'ireland', 'irish', 'www', 'http', 'https', 'com', \n",
    "    'copyright', 'privacy', 'policy', 'terms', 'conditions', 'login', 'sign', 'register',\n",
    "    \n",
    "    # Scraping / HTML Artifacts\n",
    "    'page', 'section', 'footer', 'header', 'sidebar', 'widget', 'nav', 'advertisement', 'ad',\n",
    "    'promo', 'cookie', 'script', 'javascript', 'css', 'html', 'body', 'main', 'published', 'updated',\n",
    "    'author', 'post', 'article', 'url', 'permalink',\n",
    "    \n",
    "    # Generic Advice / High Frequency Verbs (Noise for LDA)\n",
    "    'day', 'new', 'good', 'bad',\n",
    "    'check', 'try', 'keep',\n",
    "    'like', 'just', 'get', 'also', 'would', 'could', 'one', 'make', 'use', 'way', 'well', \n",
    "    'time', 'know', 'need', 'really', 'thing', 'think', 'much', 'even', 'still', 'another', \n",
    "    'every', 'go', 'want', 'take', 'find', 'look', 'come', 'year', 'old', 'may', 'might',\n",
    "    \n",
    "    # Interaction / Navigation\n",
    "    'click', 'read', 'link', 'menu', 'comment', 'reply',\n",
    "    \n",
    "    # Text Slang / Filler\n",
    "    'u', 'ur', 'im', 'dont', 'cant', 'wont', 'oh', 'ok', 'please', 'thanks', 'thank', 'yes', 'no'\n",
    "]\n",
    "\n",
    "# Merge them\n",
    "all_stopwords = stop_words.union(domain_stopwords)\n",
    "\n",
    "# --- 2. SETUP LEMMATIZER & TAGGING ---\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Maps NLTK POS tags to WordNet POS tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# ADVANCED CLEANING FUNCTION \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies regex, lemmatization, and stopword removal.\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove URLs and HTML\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # 3. Remove Page Separators (Common in scraped data)\n",
    "    text = re.sub(r'[\\_\\-\\.]{1,}', ' ', text)\n",
    "    text = re.sub(r'[/\\\\]', ' ', text)\n",
    "\n",
    "    # 4. Remove Apostrophes (don't -> dont) for easier tokenization\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    \n",
    "    # 5. Remove non-letters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 6. Remove single chars\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Tokenize\n",
    "    words = text.split()\n",
    "    \n",
    "    # 8. POS Tagging + Lemmatization\n",
    "    # (We do this to ensure \"running\" becomes \"run\", but \"runner\" stays \"runner\")\n",
    "    tagged_words = pos_tag(words)\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    # 9. Remove Stopwords (using the merged list)\n",
    "    # We also ensure words are longer than 2 chars\n",
    "    filtered_words = [word for word in lemmatized_words if word not in all_stopwords and len(word) > 2]\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "# APPLY CLEANING TO DATA\n",
    "\n",
    "print(f\"Cleaning {len(df)} documents (this may take a moment due to POS tagging)...\")\n",
    "\n",
    "# Apply the function to the Content column\n",
    "# result is a list of lists: [['word', 'noun'], ['another', 'list']]\n",
    "texts = df['Content'].apply(clean_text).tolist()\n",
    "\n",
    "# Filter out empty documents (documents that became empty after cleaning)\n",
    "texts = [t for t in texts if len(t) > 0]\n",
    "print(f\"Valid documents after cleaning: {len(texts)}\")\n",
    "\n",
    "\n",
    "# CREATE DICTIONARY & FILTER\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# FILTERING COMMON WORDS \n",
    "# This is where we enforce the \"Common Words\" rule mathematically.\n",
    "# 1. no_below=5: Word must appear in at least 5 docs (removes typos/rare names)\n",
    "# 2. no_above=0.4: Word cannot appear in more than 40% of docs.\n",
    "#    (Lowering this from 0.5 removes more \"glue\" words that weren't in your custom list)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.4)\n",
    "\n",
    "print(f\"Dictionary size after filtering: {len(dictionary)} unique tokens.\")\n",
    "\n",
    "# Create Corpus (Bag of Words)\n",
    "doc_term_matrix = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print(\"Preprocessing complete. Ready for LDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8049d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Optimal Topics (Your Step 2 logic)\n",
    "coher_vals2 = []\n",
    "lda_models = []\n",
    "limit = 15; start=2; step=1  # Using 15 instead of 25 for speed in this demo\n",
    "\n",
    "for num_topics in range(start, limit+1, step):\n",
    "    # Train LDA\n",
    "    lda_model = LdaModel(\n",
    "        corpus=doc_term_matrix,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        chunksize=2000,\n",
    "        passes=30,\n",
    "        iterations=500,\n",
    "        random_state=43,\n",
    "        update_every=1, # Update model after every chunksize docs\n",
    "        alpha='auto',   # Learn asymmetric prior from data (usually better)\n",
    "        eta='auto'      # Learn asymmetric topic-word prior\n",
    "    )\n",
    "    \n",
    "    lda_models.append(lda_model)\n",
    "    \n",
    "    # Calculate Coherence\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model, \n",
    "        texts=texts, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "    coher_vals2.append(coherence_model.get_coherence())\n",
    "    print(f\"Topics: {num_topics} | Coherence Score: {coher_vals2[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab3539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Plot Coherence Scores\n",
    "x = range(start, limit+1, step)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, coher_vals2, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "# plt.title(\"Optimal Number of Topics\")\n",
    "plt.grid(True)\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"optimal_number_of_topics.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75319826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply Best LDA Model\n",
    "print(\"\\nApplying Best Model ---\")\n",
    "\n",
    "# Find the index of the maximum coherence score\n",
    "optimal_num_topics_index = coher_vals2.index(max(coher_vals2))\n",
    "optimal_num_topics = x[optimal_num_topics_index]\n",
    "print(f\"Optimal Number of Topics: {optimal_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model from our list\n",
    "best_lda_model = lda_models[optimal_num_topics_index]\n",
    "\n",
    "# Print the topics\n",
    "# Each topic is a list of (word, probability) tuples\n",
    "print(f\"\\nTop 10 words for each of the {optimal_num_topics} topics:\")\n",
    "topics = best_lda_model.print_topics(num_words=10)\n",
    "for topic_id, topic_content in topics:\n",
    "    print(f\"Topic {topic_id}: {topic_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
