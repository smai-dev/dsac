{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import make_scorer, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # Required for 3D projection\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.data.path.append(\"./data\")\n",
    "nltk.download('punkt', download_dir=\"./data\")\n",
    "nltk.download(\"punkt_tab\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=\"./data\")\n",
    "nltk.download('stopwords', download_dir=\"./data\")\n",
    "nltk.download('wordnet', download_dir=\"./data\")\n",
    "\n",
    "import text_mining_utils as tmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ee09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"./data/spunout_data.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Ensure titles are strings and handle potential NaN values by replacing them with empty strings\n",
    "df['Title'] = df['Title'].fillna('').astype(str)\n",
    "\n",
    "# Remove rows where content extraction failed (marked as 'N/A' by the scraper)\n",
    "# Also drop rows where Content is actually NaN\n",
    "df = df[df['Content'] != 'N/A']\n",
    "df = df.dropna(subset=['Content'])\n",
    "\n",
    "# Concatenate the Title with the Content\n",
    "# We add a space in between to prevent the last word of the title merging with the first word of the body\n",
    "df['Content'] = df['Title'] + \" \" + df['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "# POS Tag Mapping Helper\n",
    "# NLTK's POS tagger returns \"Treebank\" tags (e.g., NN, VB, JJ).\n",
    "# The WordNetLemmatizer requires \"WordNet\" constants (e.g., 'n', 'v', 'a').\n",
    "# We need a function to map between them.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Maps NLTK POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # Default to Noun if unknown (handles many abbreviations/slang)\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the Standard Library Stopwords (SpaCy is preferred over NLTK as it is more comprehensive)\n",
    "# This automatically covers:\n",
    "# - Pronouns (\"he\", \"she\", \"they\", \"its\", \"whose\", \"whom\")\n",
    "# - Determiners (\"this\", \"that\", \"these\", \"those\")\n",
    "# - Basic function words\n",
    "stop_words = set(SPACY_STOP_WORDS)\n",
    "\n",
    "# Extended Domain-Specific Stopwords for SpunOut.ie\n",
    "# Includes site-specific noise, generic web terms, and common filler words.\n",
    "domain_stopwords = [\n",
    "    # Site & Web specific (Libraries don't know 'spunout' is noise)\n",
    "    'spunout', 'spun', 'out', 'ie', 'ireland', 'irish', 'www', 'http', 'https', 'com', \n",
    "    'copyright', 'privacy', 'policy', 'terms', 'conditions', 'login', 'sign', 'register',\n",
    "    \n",
    "    # Scraping / HTML / Metadata Artifacts\n",
    "    'page', 'section', 'footer', 'header', 'sidebar', 'widget', 'nav', 'advertisement', 'ad',\n",
    "    'promo', 'cookie', 'script', 'javascript', 'css', 'html', 'body', 'main', 'published', 'updated',\n",
    "    'author', 'post', 'article', 'url', 'permalink',\n",
    "    \n",
    "    # Generic Advice / High Frequency Verbs (Context specific noise)\n",
    "    # Libraries consider these content words, but in an advice corpus they are fillers\n",
    "    'day', 'new', 'good', 'bad',\n",
    "    'check', 'try', 'keep',\n",
    "    'like', 'just', 'get', 'also', 'would', 'could', 'one', 'make', 'use', 'way', 'well', \n",
    "    'time', 'know', 'need', 'really', 'thing', 'think', 'much', 'even', 'still', 'another', \n",
    "    'every', 'go', 'want', 'take', 'find', 'look', 'come', 'year', 'old', 'may', 'might',\n",
    "    \n",
    "    # Interaction / Navigation\n",
    "    'click', 'read', 'link', 'menu', 'comment', 'reply',\n",
    "    \n",
    "    # Text Slang / Filler\n",
    "    'u', 'ur', 'im', 'dont', 'cant', 'wont', 'oh', 'ok', 'please', 'thanks', 'thank', 'yes', 'no'\n",
    "]\n",
    "\n",
    "# Merge the standard library list with your custom list\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# DATQ CLEANING FUNCTION\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Refined text cleaning function.\n",
    "    Includes explicit removal of separator artifacts (e.g., ___, ///), \n",
    "    HTML/URL removal, Lemmatization, and Stopword filtering.\n",
    "    \"\"\"\n",
    "    # 1. Ensure text is string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove URLs and HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # 3. NEW: Explicitly remove common page break/artifact separators\n",
    "    # Matches 2 or more underscores, dashes, or dots (e.g., \"___\", \"---\", \"...\")\n",
    "    text = re.sub(r'[\\_\\-\\.]{1,}', ' ', text)\n",
    "    # Remove standalone slashes (forward or backward)\n",
    "    text = re.sub(r'[/\\\\]', ' ', text)\n",
    "\n",
    "    # 4. Remove apostrophes to unify contractions (e.g., \"don't\" -> \"dont\")\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    \n",
    "    # 5. Remove all non-letter characters (except spaces) - Final Polish\n",
    "    # This removes remaining symbols like @, #, $, %, &, *, etc.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 6. Remove single characters that are surrounded by spaces\n",
    "    # This cleans up leftover fragments like \" a \" or \" b \" that usually hold no meaning\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # 8. POS Tagging\n",
    "    # Creates a list of tuples: [('checking', 'VBG'), ('email', 'NN')]\n",
    "    # Note: This step is the slowest part of the pipeline.\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # 9. Context-Aware Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_words:\n",
    "        # Map the Treebank tag to WordNet tag\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        \n",
    "        # Lemmatize using the specific Part of Speech\n",
    "        # If the word is \"running\" and tag is Verb, it becomes \"run\"\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    # 10. Filter Stopwords and Short Words\n",
    "    # We filter AFTER lemmatization to ensure we catch the root forms\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE EXECUTION\n",
    "\n",
    "# Apply Text Cleaning \n",
    "print(\"Applying Text Cleaning (POS + Lemmatization)...\")\n",
    "df['Clean_Content'] = df['Content'].apply(clean_text)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "# X_raw represents the raw combined text data\n",
    "# X represents the preprocessed text data\n",
    "X_raw = df['Content']\n",
    "X = df['Clean_Content']\n",
    "# y represents the labels (categories) for validation/comparison\n",
    "y = df['Category']\n",
    "t = df['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF VECTORIZATION\n",
    "print(\"Vectorizing with TF-IDF...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with advanced parameters\n",
    "# We use the 'stop_words' variable created in your preprocessing step\n",
    "# (which merged standard English words with your custom domain list)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    # Increase max_features to 20,000 to accommodate the explosion of trigrams\n",
    "    max_features=20000,           \n",
    "    \n",
    "    # Expand to Trigrams: Essential for separating specific phrases \n",
    "    # (e.g., distinguishing \"mental health\" from \"mental health act\")\n",
    "    ngram_range=(1, 3),\n",
    "    \n",
    "    # Frequency Filtering: \n",
    "    # min_df=3 removes noise/typos (words appearing in fewer than 3 docs)\n",
    "    min_df=3,                     \n",
    "    \n",
    "    # max_df=0.7 removes \"glue\" words appearing in more than 70% of docs\n",
    "    max_df=0.7,                    \n",
    "    \n",
    "    # Logarithmic scaling: Dampens the effect of words appearing 100 times vs 10 times\n",
    "    sublinear_tf=True,            \n",
    "    \n",
    "    # Use L2 Norm: Ensures all document vectors have the same length\n",
    "    norm='l2'                     \n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned content\n",
    "X_tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"TF-IDF Shape: {X_tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(X, variance_threshold=0.90, max_components=5000):\n",
    "    \"\"\"\n",
    "    Finds the smallest number of components (K) that explains \n",
    "    a specific percentage of the variance in the data.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Set a safe upper limit for the search (to prevent memory crashes)\n",
    "    # Usually we don't need more components than we have samples\n",
    "    search_limit = min(n_samples, max_components, n_features)\n",
    "    \n",
    "    print(f\"Analyzing variance for up to {search_limit} components...\")\n",
    "    \n",
    "    # 1. Fit SVD with the search limit\n",
    "    svd_analyzer = TruncatedSVD(n_components=search_limit, random_state=42)\n",
    "    svd_analyzer.fit(X)\n",
    "    \n",
    "    # 2. Calculate Cumulative Explained Variance\n",
    "    # This array tells us how much \"information\" is preserved as we add components\n",
    "    explained_variance_ratio = svd_analyzer.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # 3. Find the 'k' where we hit the variance threshold\n",
    "    # We add 1 because indices are 0-based\n",
    "    k_indices = np.where(cumulative_variance >= variance_threshold)[0]\n",
    "    \n",
    "    if len(k_indices) > 0:\n",
    "        optimal_k = k_indices[0] + 1\n",
    "        variance_acquired = cumulative_variance[optimal_k - 1]\n",
    "    else:\n",
    "        # If we didn't reach the threshold within the limit, use the max limit\n",
    "        optimal_k = search_limit\n",
    "        variance_acquired = cumulative_variance[-1]\n",
    "        print(f\"Warning: Threshold {variance_threshold*100}% not reached within {search_limit} components.\")\n",
    "\n",
    "\n",
    "    print(f\"Target Variance:   {variance_threshold*100}%\")\n",
    "    print(f\"Optimal K found:   {optimal_k}\")\n",
    "    print(f\"Actual Variance:   {variance_acquired:.4f} ({variance_acquired*100:.2f}%)\")\n",
    "\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "\n",
    "\n",
    "# Find the best K automatically (aiming for 90% variance)\n",
    "# You can adjust variance_threshold to 0.95 for stricter precision, or 0.85 for speed\n",
    "best_k = find_optimal_k(X_tfidf_matrix, variance_threshold=0.90)\n",
    "\n",
    "# Apply TruncatedSVD with the calculated best_k\n",
    "print(f\"\\nApplying TruncatedSVD with K={best_k}...\")\n",
    "svd_final = TruncatedSVD(n_components=best_k, random_state=42)\n",
    "\n",
    "X_tfidf_reduced_matrix = svd_final.fit_transform(X_tfidf_matrix)\n",
    "\n",
    "print(f\"Final Reduced Matrix Shape: {X_tfidf_reduced_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf60c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# SHAPE THE TRAINING DATA: Normalize to Unit Length\n",
    "# This converts Euclidean distance into Cosine Similarity logic.\n",
    "# It prevents \"Big\" clusters from swallowing \"Small\" ones due to magnitude.\n",
    "normalizer = Normalizer()\n",
    "X_tfidf_matrix_normalized = normalizer.fit_transform(X_tfidf_reduced_matrix)\n",
    "\n",
    "print(\"\\nData normalized for Spherical K-Means...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cec048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "db_scores = []\n",
    "sil_scores = []\n",
    "\n",
    "# Use normalized data from your pipeline\n",
    "X_data = X_tfidf_matrix_normalized.copy()\n",
    "\n",
    "print(\"Evaluating Agglomerative Clustering from K=2 to K=10...\\n\")\n",
    "\n",
    "for k in range(2, 10):\n",
    "    # Initialize Agglomerative clustering\n",
    "    agg = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        metric='euclidean',\n",
    "        linkage='ward'    # good default — requires euclidean\n",
    "    )\n",
    "    \n",
    "    # Fit & predict labels\n",
    "    labels = agg.fit_predict(X_data)\n",
    "    \n",
    "    # Compute metrics\n",
    "    db_index = davies_bouldin_score(X_data, labels)\n",
    "    sil_score = silhouette_score(X_data, labels)\n",
    "    \n",
    "    db_scores.append(db_index)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "    print(f\"K={k}: Davies-Bouldin={db_index:.4f} | Silhouette={sil_score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(range(2, 10), db_scores, marker='o', label='Davies-Bouldin', color='red')\n",
    "ax.plot(range(2, 10), sil_scores, marker='x', label='Silhouette', color='blue')\n",
    "\n",
    "ax.set_xlabel('Number of clusters (k)')\n",
    "ax.set_ylabel('Score')\n",
    "# ax.set_title('Agglomerative Clustering Validation Metrics')\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"agglomerative_clustering_clusters_dbouldin_silhouette_euclidean.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lists to store evaluation metrics\n",
    "db_scores = []\n",
    "sil_scores = []\n",
    "\n",
    "# Use normalized data from your pipeline\n",
    "X_data = X_tfidf_matrix_normalized.copy()\n",
    "\n",
    "print(\"Evaluating Agglomerative Clustering from K=2 to K=10...\\n\")\n",
    "\n",
    "for k in range(2, 10):\n",
    "    # Initialize Agglomerative clustering\n",
    "    agg = AgglomerativeClustering(\n",
    "        n_clusters=k,\n",
    "        metric='cosine',\n",
    "        linkage='complete'    # good for cosine — requires pre-normalized data\n",
    "    )\n",
    "    \n",
    "    # Fit & predict labels\n",
    "    labels = agg.fit_predict(X_data)\n",
    "    \n",
    "    # Compute metrics\n",
    "    db_index = davies_bouldin_score(X_data, labels)\n",
    "    sil_score = silhouette_score(X_data, labels)\n",
    "    \n",
    "    db_scores.append(db_index)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "    print(f\"K={k}: Davies-Bouldin={db_index:.4f} | Silhouette={sil_score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(range(2, 10), db_scores, marker='o', label='Davies-Bouldin', color='red')\n",
    "ax.plot(range(2, 10), sil_scores, marker='x', label='Silhouette', color='blue')\n",
    "\n",
    "ax.set_xlabel('Number of clusters (k)')\n",
    "ax.set_ylabel('Score')\n",
    "# ax.set_title('Agglomerative Clustering Validation Metrics')\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"agglomerative_clustering_clusters_dbouldin_silhouette_cosine.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Agglomerative Clustering\n",
    "agg = AgglomerativeClustering(\n",
    "    n_clusters=7,\n",
    "    metric='euclidean',\n",
    "    linkage='ward'    # good default — requires euclidean\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    # 1. compute_distances: \n",
    "    # Calculates the distances between merged clusters. \n",
    "    # False = Faster (Default)\n",
    "    # True  = Required if you want to plot a Dendrogram later\n",
    "    'compute_distances': [False, True],\n",
    "    \n",
    "    # 2. compute_full_tree:\n",
    "    # Determines if the algorithm stops merging as soon as it hits 7 clusters ('auto') \n",
    "    # or builds the entire hierarchy first (True).\n",
    "    # 'auto' = Faster, lower memory\n",
    "    # True   = Slower, higher memory, necessary for very specific distance calculations\n",
    "    'compute_full_tree': ['auto', True]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "# Note: AgglomerativeClustering is memory-intensive. \n",
    "# If this crashes with MemoryError, reduce 'cv' to 3.\n",
    "print(\"Running Grid Search for Agglomerative Clustering...\")\n",
    "print(\"Note: This method merges all data (no noise) but can be slower on large datasets.\")\n",
    "\n",
    "agg_grid_search = GridSearchCV(\n",
    "    agg, \n",
    "    param_grid=param_grid, \n",
    "    scoring=make_scorer(silhouette_score),\n",
    "    cv=3,         # 3-fold Cross Validation\n",
    "    n_jobs=-1,    # Use all cores (GridSearchCV level)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on NORMALIZED data\n",
    "agg_grid_search.fit(X_tfidf_matrix_normalized)\n",
    "\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(agg_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1064d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Final Model\n",
    "best_agg = agg_grid_search.best_estimator_\n",
    "\n",
    "# Convert sparse TF-IDF to a dense DataFrame (optional: keep sparse for memory efficiency)\n",
    "agg_results_df = pd.DataFrame(X_tfidf_matrix_normalized)\n",
    "\n",
    "# Add cluster labels\n",
    "agg_results_df['cluster_label'] = best_agg.labels_\n",
    "\n",
    "# Add true original website labels (Category)\n",
    "agg_results_df['true_label'] = y\n",
    "\n",
    "# Print how clusters correspond to categories\n",
    "print(\"\\nCluster Composition:\")\n",
    "print(agg_results_df.groupby('cluster_label')['true_label'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30828fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Final Model\n",
    "best_agg = agg_grid_search.best_estimator_\n",
    "\n",
    "# Convert sparse TF-IDF to a dense DataFrame (optional: keep sparse for memory efficiency)\n",
    "agg_results_df = pd.DataFrame(X_tfidf_matrix_normalized)\n",
    "\n",
    "# Add cluster labels\n",
    "agg_results_df['cluster_label'] = best_agg.labels_\n",
    "\n",
    "# Add true original website labels (Category)\n",
    "agg_results_df['true_label'] = y\n",
    "\n",
    "agg_results_df['topics'] = t\n",
    "\n",
    "# Group by the three dimensions and count the occurrences\n",
    "composition = agg_results_df.groupby(['cluster_label', 'true_label', 'topics']).size()\n",
    "\n",
    "print(\"\\nDetailed Cluster Composition (Cluster > Category > Topic):\")\n",
    "print(composition.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a38abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"\\nGenerating 3D Cluster Visualization with Plotly...\")\n",
    "\n",
    "# Reduce dimensions to 3D\n",
    "svd = TruncatedSVD(n_components=3, random_state=43)\n",
    "X_svd = svd.fit_transform(X_tfidf_matrix_normalized)\n",
    "\n",
    "# Create DataFrame\n",
    "plot_df = pd.DataFrame(X_svd, columns=['Component 1', 'Component 2', 'Component 3'])\n",
    "\n",
    "# FIX: Keep labels as INTEGERS (numbers), not strings\n",
    "# This allows us to use the Viridis gradient\n",
    "plot_df['Cluster'] = best_agg.labels_\n",
    "\n",
    "# Plot using scatter_3d\n",
    "fig = px.scatter_3d(\n",
    "    plot_df,\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    z='Component 3',\n",
    "    color='Cluster',\n",
    "    title=f'Interactive 3D Agglomerative Clusters<br><sub>TruncatedSVD Projection of TF-IDF Matrix</sub>',\n",
    "    opacity=0.7,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    # FIX: Use color_continuous_scale for gradients like Viridis\n",
    "    color_continuous_scale='viridis'\n",
    ")\n",
    "\n",
    "# Update the layout for cleaner axis labels\n",
    "fig.update_traces(marker=dict(size=3, line=dict(width=0)))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Component 1',\n",
    "        yaxis_title='Component 2',\n",
    "        zaxis_title='Component 3'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=50) # Tight layout\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5834b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Number of clusters\n",
    "n_clusters = best_agg.n_clusters  # this = 7\n",
    "\n",
    "#  Dynamically determine grid size\n",
    "cols = 2\n",
    "rows = math.ceil(n_clusters / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 5 * rows))\n",
    "fig.suptitle('Top 20 Words per Cluster (by TF-IDF Score)', fontsize=20)\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # 1. Docs in this cluster\n",
    "    cluster_indices = np.where(best_agg.labels_ == i)[0]\n",
    "    \n",
    "    # 2. TF-IDF rows for cluster\n",
    "    cluster_tfidf = X_tfidf_matrix[cluster_indices]\n",
    "    \n",
    "    # 3. Sum TF-IDF scores\n",
    "    word_scores = np.array(cluster_tfidf.sum(axis=0)).flatten()\n",
    "    \n",
    "    # 4. Top 20 words\n",
    "    top_indices = word_scores.argsort()[-20:][::-1]\n",
    "    top_words = feature_names[top_indices]\n",
    "    top_scores = word_scores[top_indices]\n",
    "    \n",
    "    # 5. DataFrame\n",
    "    df_plot = pd.DataFrame({\"Word\": top_words, \"Score\": top_scores})\n",
    "\n",
    "    # 6. Plot\n",
    "    sns.barplot(ax=axes[i], data=df_plot, x='Score', y='Word', palette='viridis')\n",
    "    axes[i].set_title(f'Cluster {i}', fontsize=14)\n",
    "    axes[i].set_xlabel('Total TF-IDF Score')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "# Hide extra empty subplots\n",
    "for j in range(n_clusters, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"agglomerative_top_20_words_per_cluster.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36290c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Words per Cluster (by TF-IDF Score):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # (Steps A, B, C, D from your original code remain the same)\n",
    "    cluster_indices = np.where(best_agg.labels_ == i)[0]\n",
    "    cluster_tfidf = X_tfidf_matrix[cluster_indices]\n",
    "    word_scores = np.array(cluster_tfidf.sum(axis=0)).flatten()\n",
    "    top_indices = word_scores.argsort()[-20:][::-1]\n",
    "    \n",
    "    top_words = feature_names[top_indices]\n",
    "    top_scores = word_scores[top_indices]\n",
    "    \n",
    "    # Text display logic\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    # Join words with commas for a compact view\n",
    "    print(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate centroids for all clusters\n",
    "centroids = []\n",
    "for i in range(n_clusters):\n",
    "    mask = (best_agg.labels_ == i)\n",
    "    centroids.append(X_tfidf_matrix_normalized[mask].mean(axis=0))\n",
    "\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# Compute similarity matrix between centroids\n",
    "centroid_similarity = cosine_similarity(centroids)\n",
    "\n",
    "# Plot as a Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    centroid_similarity, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    xticklabels=[f'C{i}' for i in range(n_clusters)],\n",
    "    yticklabels=[f'C{i}' for i in range(n_clusters)],\n",
    "    vmin=0, vmax=1\n",
    ")\n",
    "#plt.title('Inter-Cluster Similarity (Topic Overlap)')\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"agglomerative_inter_cluster_similarity.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa13403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the clusters (e.g., 'Cluster 0', 'Cluster 1', etc.)\n",
    "cluster_labels = [f'Cluster {i}' for i in range(n_clusters)]\n",
    "\n",
    "# Convert the numpy array into a Pandas DataFrame\n",
    "# We use the labels for both the Index (rows) and Columns\n",
    "df_similarity = pd.DataFrame(\n",
    "    centroid_similarity, \n",
    "    index=cluster_labels, \n",
    "    columns=cluster_labels\n",
    ")\n",
    "\n",
    "# Round the values to 3 decimal places for readability\n",
    "df_similarity = df_similarity.round(3)\n",
    "\n",
    "# Print the text-based table\n",
    "print(\"\\nInter-Cluster Similarity Matrix:\")\n",
    "print(df_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ba235",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifying potential Outliers (weakest links)...\\n\")\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_mask = (best_agg.labels_ == i)\n",
    "    cluster_docs = X_tfidf_matrix_normalized[cluster_mask]\n",
    "    \n",
    "    # FIX: Reshape the centroid to be 2D (1 row, n_features columns)\n",
    "    # .mean(axis=0) creates a 1D array. .reshape(1, -1) converts it to 2D.\n",
    "    centroid = cluster_docs.mean(axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(cluster_docs, centroid).flatten()\n",
    "    \n",
    "    # Find the document with the LOWEST similarity to the center\n",
    "    worst_doc_local_index = similarities.argsort()[0]\n",
    "    \n",
    "    original_df_indices = np.where(best_agg.labels_ == i)[0]\n",
    "    worst_original_index = original_df_indices[worst_doc_local_index]\n",
    "    \n",
    "    score = similarities[worst_doc_local_index]\n",
    "    \n",
    "    # Only print if the document is really far away (e.g., similarity < 0.1)\n",
    "    if score < 0.1: \n",
    "        title = df['Title'].iloc[worst_original_index]\n",
    "        print(f\"CLUSTER {i} Outlier (Score: {score:.4f}):\")\n",
    "        print(f\"  > {title}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
