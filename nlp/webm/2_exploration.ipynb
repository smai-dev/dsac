{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import text_mining_utils as tmu\n",
    "\n",
    "nltk.data.path.append(\"./data\")\n",
    "nltk.download('punkt', download_dir=\"./data\")\n",
    "nltk.download(\"punkt_tab\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=\"./data\")\n",
    "nltk.download('stopwords', download_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ee27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/spunout_data.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Ensure titles are strings and handle potential NaN values by replacing them with empty strings\n",
    "df['Title'] = df['Title'].fillna('').astype(str)\n",
    "\n",
    "# Remove rows where content extraction failed (marked as 'N/A' by the scraper)\n",
    "# Also drop rows where Content is actually NaN\n",
    "df = df[df['Content'] != 'N/A']\n",
    "df = df.dropna(subset=['Content'])\n",
    "\n",
    "# Concatenate the Title with the Content\n",
    "# We add a space in between to prevent the last word of the title merging with the first word of the body\n",
    "df['Content'] = df['Title'] + \" \" + df['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "nltk.download('wordnet', download_dir=\"./data\")\n",
    "\n",
    "# Text Initial Preprocessing\n",
    "\n",
    "# Load the Standard Library Stopwords (SpaCy is preferred over NLTK as it is more comprehensive)\n",
    "# This automatically covers:\n",
    "# - Pronouns (\"he\", \"she\", \"they\", \"its\", \"whose\", \"whom\")\n",
    "# - Determiners (\"this\", \"that\", \"these\", \"those\")\n",
    "# - Basic function words\n",
    "stop_words = set(SPACY_STOP_WORDS)\n",
    "\n",
    "# Extended Domain-Specific Stopwords for SpunOut.ie\n",
    "# Includes site-specific noise, generic web terms, and common filler words.\n",
    "domain_stopwords = [\n",
    "    # Site & Web specific (Libraries don't know 'spunout' is noise)\n",
    "    'spunout', 'spun', 'out', 'ie', 'ireland', 'irish', 'www', 'http', 'https', 'com', \n",
    "    'copyright', 'privacy', 'policy', 'terms', 'conditions', 'login', 'sign', 'register',\n",
    "    \n",
    "    # Scraping / HTML / Metadata Artifacts\n",
    "    'page', 'section', 'footer', 'header', 'sidebar', 'widget', 'nav', 'advertisement', 'ad',\n",
    "    'promo', 'cookie', 'script', 'javascript', 'css', 'html', 'body', 'main', 'published', 'updated',\n",
    "    'author', 'post', 'article', 'url', 'permalink',\n",
    "    \n",
    "    # Generic Advice / High Frequency Verbs (Context specific noise)\n",
    "    # Libraries consider these content words, but in an advice corpus they are fillers\n",
    "    'day', 'new', 'good', 'bad',\n",
    "    'check', 'try', 'keep',\n",
    "    'like', 'just', 'get', 'also', 'would', 'could', 'one', 'make', 'use', 'way', 'well', \n",
    "    'time', 'know', 'need', 'really', 'thing', 'think', 'much', 'even', 'still', 'another', \n",
    "    'every', 'go', 'want', 'take', 'find', 'look', 'come', 'year', 'old', 'may', 'might',\n",
    "    \n",
    "    # Interaction / Navigation\n",
    "    'click', 'read', 'link', 'menu', 'comment', 'reply',\n",
    "    \n",
    "    # Text Slang / Filler\n",
    "    'u', 'ur', 'im', 'dont', 'cant', 'wont', 'oh', 'ok', 'please', 'thanks', 'thank', 'yes', 'no'\n",
    "]\n",
    "\n",
    "# Merge the standard library list with your custom list\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Refined text cleaning function.\n",
    "    Includes explicit removal of separator artifacts (e.g., ___, ///), \n",
    "    HTML/URL removal, Lemmatization, and Stopword filtering.\n",
    "    \"\"\"\n",
    "    # 1. Ensure text is string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove URLs and HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # 3. NEW: Explicitly remove common page break/artifact separators\n",
    "    # Matches 2 or more underscores, dashes, or dots (e.g., \"___\", \"---\", \"...\")\n",
    "    text = re.sub(r'[\\_\\-\\.]{1,}', ' ', text)\n",
    "    # Remove standalone slashes (forward or backward)\n",
    "    text = re.sub(r'[/\\\\]', ' ', text)\n",
    "\n",
    "    # 4. Remove apostrophes to unify contractions (e.g., \"don't\" -> \"dont\")\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    \n",
    "    # 5. Remove all non-letter characters (except spaces) - Final Polish\n",
    "    # This removes remaining symbols like @, #, $, %, &, *, etc.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 6. Remove single characters that are surrounded by spaces\n",
    "    # This cleans up leftover fragments like \" a \" or \" b \" that usually hold no meaning\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # 8. Lemmatization (Morphological reduction)\n",
    "    # Reduces words to base form (e.g., \"studies\" -> \"study\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # 9. Filter Stopwords and Short Words\n",
    "    # Remove stopwords and words shorter than 3 characters\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "print(\"Cleaning text data (Refined: Separators Removed + Lemmatization + Extended Stopwords)\")\n",
    "df['Clean_Content'] = df['Content'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941bdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df['Word_Count'] = df['Clean_Content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "total_words = df['Word_Count'].sum()\n",
    "avg_words_per_article = df['Word_Count'].mean()\n",
    "unique_categories = df['Category'].nunique()\n",
    "unique_topics = df['Topic'].nunique()\n",
    "\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"Total categories: {unique_categories}\")\n",
    "print(f\"Total topics: {unique_topics}\")\n",
    "print(f\"Total words (after cleaning): {total_words}\")\n",
    "print(f\"Average words per article: {avg_words_per_article:.2f}\")\n",
    "print(\"\\nTop 5 categories by article count:\")\n",
    "print(df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the save path\n",
    "save_path = 'practical_assessment_adsah_6014_2_web_content_mining/images/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Visualizations \n",
    "# Set general style for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Distribution of article lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Word_Count'], bins=30, kde=True, color='skyblue')\n",
    "# plt.title('Distribution of Article Lengths (Word Count)')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(os.path.join(save_path, 'distribution_article_lengths.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Article count per category\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Limit to top 10 categories to keep the chart readable\n",
    "top_categories = df['Category'].value_counts().nlargest(10).index\n",
    "sns.countplot(data=df[df['Category'].isin(top_categories)], x='Category', order=top_categories, palette='viridis')\n",
    "# plt.title('Top 10 Categories by Article Volume')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_path, 'top_10_categories.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a805d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds per Category (for the top 3 categories) ---\n",
    "print(\"Generating Word Clouds for top categories...\")\n",
    "top_3_cats = df['Category'].value_counts().nlargest(3).index\n",
    "# Create a figure with 1 row and 3 columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "# Flatten the axes array for easy iteration (makes axes[0], axes[1], etc. work easily)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the top 3 categories and the corresponding axes\n",
    "for i, cat in enumerate(top_3_cats):\n",
    "    # 1. Prepare the text\n",
    "    cat_text = \" \".join(df[df['Category'] == cat]['Clean_Content'])\n",
    "    # 2. Generate the Word Cloud\n",
    "    # You can customize width/height/background_color here if needed\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cat_text)\n",
    "    # 3. Plot on the specific axis\n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f'{cat.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "    axes[i].axis('off')  # Hide axis ticks/lines\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "save_path = 'practical_assessment_adsah_6014_2_web_content_mining/images'\n",
    "plt.savefig(f'{save_path}/top_3_wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all words into a single list\n",
    "all_words_list = \" \".join(df['Clean_Content']).split()\n",
    "word_counts = Counter(all_words_list)\n",
    "most_common_words = word_counts.most_common(20)\n",
    "words, counts = zip(*most_common_words)\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(x=list(words), y=list(counts), palette='rocket')\n",
    "# plt.title('Top 20 Most Frequent Words in Dataset')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "# ha='right' ensures the text aligns nicely at the end of the label\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout() \n",
    "plt.savefig(os.path.join(save_path, 'top_20_frequent_words.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
