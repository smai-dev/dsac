{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f9549f",
   "metadata": {},
   "source": [
    "**Python Web Scraper Overview (spunout.ie)**\n",
    "\n",
    "This script scrapes articles from **spunout.ie** using a **Two-Pass System**; first collecting article URLs and metadata, then extracting full article text.\n",
    "\n",
    "**Libraries Used**\n",
    "\n",
    "1. **Selenium** – Automates a real browser to load JavaScript-rendered site content.\n",
    "2. **Pandas** – Structures scraped data and exports to CSV.\n",
    "3. **Time** – Adds short delays to avoid overwhelming the server.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "1. **Setup**\n",
    "\n",
    "   * Launches a **headless Chrome** browser with stability flags for server-safe execution.\n",
    "2. **URL Discovery**\n",
    "\n",
    "   * Starts on `/information`, finds all `/category/` links, removes duplicates.\n",
    "3. **Metadata Collection**\n",
    "\n",
    "   * Iterates category pages, handles pagination, extracts **Title** and **URL**\n",
    "   * Determines **Category** and **Topic** from URL format\n",
    "   * Uses a tracking set to avoid duplicate URLs\n",
    "4. **Content Extraction**\n",
    "\n",
    "   * Opens each article\n",
    "   * Waits for content to fully load\n",
    "   * Cleans extracted text\n",
    "5. **Export**\n",
    "\n",
    "   * Saves results to `spunout_data.csv` using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a205319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Chrome options to configure the browser behavior\n",
    "options = Options()\n",
    "# Run the browser in headless mode, meaning it operates without a visible graphical interface\n",
    "options.add_argument(\"--headless\")\n",
    "# Bypass the OS security model; often required for running in certain environments like Docker\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "# Overcome limited resource problems in containers by disabling shared memory usage\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "# Create the WebDriver instance with the specified options\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles, categories, topics, texts, urls_to_visit = [], [], [], [], []\n",
    "# Initialize a set to keep track of URLs we have already processed to avoid duplicates\n",
    "seen_urls = set()\n",
    "\n",
    "# Phase 1: Discover all listing pages (Main categories and sub-categories)\n",
    "print(\"Step 1: Finding all listing pages...\")\n",
    "# Navigate to the main information hub of the website\n",
    "driver.get(\"https://spunout.ie/information\")\n",
    "# Pause execution to allow the page to load fully\n",
    "time.sleep(3)\n",
    "# Find all anchor elements that contain '/category/' in their href attribute\n",
    "all_links = driver.find_elements(By.XPATH, \"//a[contains(@href, '/category/')]\")\n",
    "# Extract the href attribute from each link element and convert to a set to remove duplicates\n",
    "raw_listing_urls = list(set([el.get_attribute(\"href\") for el in all_links]))\n",
    "\n",
    "# Phase 2: Iterate through each listing page to find article URLs and metadata\n",
    "for listing_url in raw_listing_urls:\n",
    "    print(f\"\\n[SCANNING LISTING] {listing_url}\")\n",
    "    # Load the specific category or sub-category page\n",
    "    driver.get(listing_url)\n",
    "    \n",
    "    # Start a loop to handle pagination within this category\n",
    "    while True:\n",
    "        # Wait for the page to settle\n",
    "        time.sleep(2)\n",
    "        # Locate individual article blocks on the listing page\n",
    "        blocks = driver.find_elements(By.CLASS_NAME, \"news_list_single\")\n",
    "        # If no blocks are found, we have reached the end of the listing\n",
    "        if not blocks: break\n",
    "        \n",
    "        # Iterate through each article block found on the current page\n",
    "        for block in blocks:\n",
    "            try:\n",
    "                # Extract the URL of the article from the anchor tag within the block\n",
    "                article_url = block.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                \n",
    "                # Check if we have already processed this URL to prevent duplicates\n",
    "                if article_url not in seen_urls:\n",
    "                    # Extract the visible title of the article\n",
    "                    title = block.find_element(By.TAG_NAME, \"h5\").text\n",
    "                    \n",
    "                    # Metadata Extraction: Parse the URL structure to determine Category and Topic\n",
    "                    # Remove the base domain and split the remaining path by slashes\n",
    "                    path_parts = article_url.replace(\"https://spunout.ie/\", \"\").strip('/').split('/')\n",
    "                    \n",
    "                    # The Category is determined to be the first segment of the URL path\n",
    "                    # Replace hyphens with spaces, title-case it, and replace spaces with underscores for consistency\n",
    "                    final_cat = path_parts[0].replace('-', ' ').title().replace(' ', '_')\n",
    "                    \n",
    "                    # The Topic is determined to be the second segment, but only if a third segment exists (the article title)\n",
    "                    final_topic = \"\"\n",
    "                    if len(path_parts) >= 3:\n",
    "                        final_topic = path_parts[1].replace('-', ' ').title().replace(' ', '_')\n",
    "                    \n",
    "                    # Log the extracted data for verification purposes\n",
    "                    print(f\"  [ADDED] {article_url}\")\n",
    "                    print(f\"   -> Category: {final_cat} | Topic: {final_topic if final_topic else '[Empty]'}\")\n",
    "                    \n",
    "                    # Append the extracted data to their respective lists\n",
    "                    titles.append(title)\n",
    "                    categories.append(final_cat)\n",
    "                    topics.append(final_topic)\n",
    "                    urls_to_visit.append(article_url)\n",
    "                    # Mark this URL as seen\n",
    "                    seen_urls.add(article_url)\n",
    "            except:\n",
    "                # If an error occurs while processing a block, skip it and continue with the next\n",
    "                continue\n",
    "\n",
    "        # Pagination Logic: Try to find and click the 'Next' button\n",
    "        try:\n",
    "            # Locate the next page button using CSS selector\n",
    "            next_btn = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
    "            # Use JavaScript to click the element. This is often more reliable than a standard click \n",
    "            # as it bypasses visibility checks and ensures the click event fires even if the element is obscured.\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "        except:\n",
    "            # If no next button is found, exit the while loop to move to the next category\n",
    "            break\n",
    "\n",
    "# Phase 3: Content Extraction (Visit each discovered URL to get the full text)\n",
    "print(f\"\\nStep 3: Extracting text from {len(urls_to_visit)} articles...\")\n",
    "# Iterate through the list of collected article URLs using an index counter\n",
    "for i, url in enumerate(urls_to_visit):\n",
    "    try:\n",
    "        # Navigate to the specific article page\n",
    "        driver.get(url)\n",
    "        # Wait explicitly until the content div is present in the DOM\n",
    "        # This ensures that the dynamic content has loaded before we try to read it\n",
    "        content = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"content_inner\"))\n",
    "        ).text\n",
    "        # Clean up the text by splitting on whitespace and rejoining with single spaces\n",
    "        # This removes excessive newlines and tabs\n",
    "        texts.append(\" \".join(content.split()))\n",
    "        # Print progress update every 10 articles\n",
    "        if (i+1) % 10 == 0: print(f\"Progress: {i+1}/{len(urls_to_visit)}\")\n",
    "    except:\n",
    "        # If content extraction fails, append 'N/A' to maintain list alignment\n",
    "        texts.append(\"N/A\")\n",
    "\n",
    "# Close the browser window as scraping is complete\n",
    "driver.quit()\n",
    "\n",
    "# Phase 4: Final CSV Export\n",
    "# Create a Pandas DataFrame to structure the data\n",
    "df = pd.DataFrame({\n",
    "    \"Title\": titles,\n",
    "    \"Category\": categories,\n",
    "    \"Topic\": topics,\n",
    "    \"Content\": texts,\n",
    "    \"URL\": urls_to_visit\n",
    "})\n",
    "# Export the DataFrame to a CSV file\n",
    "# index=False prevents writing row numbers, encoding='utf-8-sig' ensures proper character support in Excel\n",
    "df.to_csv(\"./data/spunout_data.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"\\nSuccess! CSV saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7c9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
