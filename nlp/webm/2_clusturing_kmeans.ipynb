{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be49c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import make_scorer, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D # Required for 3D projection\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.data.path.append(\"./data\")\n",
    "nltk.download('punkt', download_dir=\"./data\")\n",
    "nltk.download(\"punkt_tab\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger\", download_dir=\"./data\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=\"./data\")\n",
    "nltk.download('stopwords', download_dir=\"./data\")\n",
    "nltk.download('wordnet', download_dir=\"./data\")\n",
    "\n",
    "import text_mining_utils as tmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c01e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/spunout_data.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Ensure titles are strings and handle potential NaN values by replacing them with empty strings\n",
    "df['Title'] = df['Title'].fillna('').astype(str)\n",
    "\n",
    "# Remove rows where content extraction failed (marked as 'N/A' by the scraper)\n",
    "# Also drop rows where Content is actually NaN\n",
    "df = df[df['Content'] != 'N/A']\n",
    "df = df.dropna(subset=['Content'])\n",
    "\n",
    "# Concatenate the Title with the Content\n",
    "# We add a space in between to prevent the last word of the title merging with the first word of the body\n",
    "df['Content'] = df['Title'] + \" \" + df['Content']\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "# POS Tag Mapping Helper\n",
    "# NLTK's POS tagger returns \"Treebank\" tags (e.g., NN, VB, JJ).\n",
    "# The WordNetLemmatizer requires \"WordNet\" constants (e.g., 'n', 'v', 'a').\n",
    "# We need a function to map between them.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Maps NLTK POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # Default to Noun if unknown (handles many abbreviations/slang)\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the Standard Library Stopwords (SpaCy is preferred over NLTK as it is more comprehensive)\n",
    "# This automatically covers:\n",
    "# - Pronouns (\"he\", \"she\", \"they\", \"its\", \"whose\", \"whom\")\n",
    "# - Determiners (\"this\", \"that\", \"these\", \"those\")\n",
    "# - Basic function words\n",
    "stop_words = set(SPACY_STOP_WORDS)\n",
    "\n",
    "# Extended Domain-Specific Stopwords for SpunOut.ie\n",
    "# Includes site-specific noise, generic web terms, and common filler words.\n",
    "domain_stopwords = [\n",
    "    # Site & Web specific (Libraries don't know 'spunout' is noise)\n",
    "    'spunout', 'spun', 'out', 'ie', 'ireland', 'irish', 'www', 'http', 'https', 'com', \n",
    "    'copyright', 'privacy', 'policy', 'terms', 'conditions', 'login', 'sign', 'register',\n",
    "    \n",
    "    # Scraping / HTML / Metadata Artifacts\n",
    "    'page', 'section', 'footer', 'header', 'sidebar', 'widget', 'nav', 'advertisement', 'ad',\n",
    "    'promo', 'cookie', 'script', 'javascript', 'css', 'html', 'body', 'main', 'published', 'updated',\n",
    "    'author', 'post', 'article', 'url', 'permalink',\n",
    "    \n",
    "    # Generic Advice / High Frequency Verbs (Context specific noise)\n",
    "    # Libraries consider these content words, but in an advice corpus they are fillers\n",
    "    'day', 'new', 'good', 'bad',\n",
    "    'check', 'try', 'keep',\n",
    "    'like', 'just', 'get', 'also', 'would', 'could', 'one', 'make', 'use', 'way', 'well', \n",
    "    'time', 'know', 'need', 'really', 'thing', 'think', 'much', 'even', 'still', 'another', \n",
    "    'every', 'go', 'want', 'take', 'find', 'look', 'come', 'year', 'old', 'may', 'might',\n",
    "    \n",
    "    # Interaction / Navigation\n",
    "    'click', 'read', 'link', 'menu', 'comment', 'reply',\n",
    "    \n",
    "    # Text Slang / Filler\n",
    "    'u', 'ur', 'im', 'dont', 'cant', 'wont', 'oh', 'ok', 'please', 'thanks', 'thank', 'yes', 'no'\n",
    "]\n",
    "\n",
    "# Merge the standard library list with your custom list\n",
    "stop_words.update(domain_stopwords)\n",
    "\n",
    "# DATQ CLEANING FUNCTION\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Refined text cleaning function.\n",
    "    Includes explicit removal of separator artifacts (e.g., ___, ///), \n",
    "    HTML/URL removal, Lemmatization, and Stopword filtering.\n",
    "    \"\"\"\n",
    "    # 1. Ensure text is string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove URLs and HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    # 3. NEW: Explicitly remove common page break/artifact separators\n",
    "    # Matches 2 or more underscores, dashes, or dots (e.g., \"___\", \"---\", \"...\")\n",
    "    text = re.sub(r'[\\_\\-\\.]{1,}', ' ', text)\n",
    "    # Remove standalone slashes (forward or backward)\n",
    "    text = re.sub(r'[/\\\\]', ' ', text)\n",
    "\n",
    "    # 4. Remove apostrophes to unify contractions (e.g., \"don't\" -> \"dont\")\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    \n",
    "    # 5. Remove all non-letter characters (except spaces) - Final Polish\n",
    "    # This removes remaining symbols like @, #, $, %, &, *, etc.\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 6. Remove single characters that are surrounded by spaces\n",
    "    # This cleans up leftover fragments like \" a \" or \" b \" that usually hold no meaning\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "    \n",
    "    # 7. Tokenize (split into words)\n",
    "    words = text.split()\n",
    "    \n",
    "    # 8. POS Tagging\n",
    "    # Creates a list of tuples: [('checking', 'VBG'), ('email', 'NN')]\n",
    "    # Note: This step is the slowest part of the pipeline.\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # 9. Context-Aware Lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_words:\n",
    "        # Map the Treebank tag to WordNet tag\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        \n",
    "        # Lemmatize using the specific Part of Speech\n",
    "        # If the word is \"running\" and tag is Verb, it becomes \"run\"\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    # 10. Filter Stopwords and Short Words\n",
    "    # We filter AFTER lemmatization to ensure we catch the root forms\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE EXECUTION\n",
    "\n",
    "# Apply Text Cleaning \n",
    "print(\"Applying Text Cleaning (POS + Lemmatization)...\")\n",
    "df['Clean_Content'] = df['Content'].apply(clean_text)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "# X_raw represents the raw combined text data\n",
    "# X represents the preprocessed text data\n",
    "X_raw = df['Content']\n",
    "X = df['Clean_Content']\n",
    "# y represents the labels (categories) for validation/comparison\n",
    "y = df['Category']\n",
    "t = df['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f84177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF VECTORIZATION\n",
    "print(\"Vectorizing with TF-IDF...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with advanced parameters\n",
    "# We use the 'stop_words' variable created in your preprocessing step\n",
    "# (which merged standard English words with your custom domain list)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    # Increase max_features to 20,000 to accommodate the explosion of trigrams\n",
    "    max_features=20000,           \n",
    "    \n",
    "    # Expand to Trigrams: Essential for separating specific phrases \n",
    "    # (e.g., distinguishing \"mental health\" from \"mental health act\")\n",
    "    ngram_range=(1, 3),\n",
    "    \n",
    "    # Frequency Filtering: \n",
    "    # min_df=3 removes noise/typos (words appearing in fewer than 3 docs)\n",
    "    min_df=3,                     \n",
    "    \n",
    "    # max_df=0.7 removes \"glue\" words appearing in more than 70% of docs\n",
    "    max_df=0.7,                    \n",
    "    \n",
    "    # Logarithmic scaling: Dampens the effect of words appearing 100 times vs 10 times\n",
    "    sublinear_tf=True,            \n",
    "    \n",
    "    # Use L2 Norm: Ensures all document vectors have the same length\n",
    "    norm='l2'                     \n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned content\n",
    "X_tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(f\"TF-IDF Shape: {X_tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the smallest number of components (K) that explains \n",
    "# a specific percentage of the variance in the data.\n",
    "def find_optimal_k(X, variance_threshold=0.90, max_components=5000):\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Set a safe upper limit for the search (to prevent memory crashes)\n",
    "    # Usually we don't need more components than we have samples\n",
    "    search_limit = min(n_samples, max_components, n_features)\n",
    "    \n",
    "    print(f\"Analyzing variance for up to {search_limit} components...\")\n",
    "    \n",
    "    # 1. Fit SVD with the search limit\n",
    "    svd_analyzer = TruncatedSVD(n_components=search_limit, random_state=42)\n",
    "    svd_analyzer.fit(X)\n",
    "    \n",
    "    # 2. Calculate Cumulative Explained Variance\n",
    "    # This array tells us how much \"information\" is preserved as we add components\n",
    "    explained_variance_ratio = svd_analyzer.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # 3. Find the 'k' where we hit the variance threshold\n",
    "    # We add 1 because indices are 0-based\n",
    "    k_indices = np.where(cumulative_variance >= variance_threshold)[0]\n",
    "    \n",
    "    if len(k_indices) > 0:\n",
    "        optimal_k = k_indices[0] + 1\n",
    "        variance_acquired = cumulative_variance[optimal_k - 1]\n",
    "    else:\n",
    "        # If we didn't reach the threshold within the limit, use the max limit\n",
    "        optimal_k = search_limit\n",
    "        variance_acquired = cumulative_variance[-1]\n",
    "        print(f\"Warning: Threshold {variance_threshold*100}% not reached within {search_limit} components.\")\n",
    "\n",
    "\n",
    "    print(f\"Target Variance:   {variance_threshold*100}%\")\n",
    "    print(f\"Optimal K found:   {optimal_k}\")\n",
    "    print(f\"Actual Variance:   {variance_acquired:.4f} ({variance_acquired*100:.2f}%)\")\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "\n",
    "\n",
    "# Find the best K automatically (aiming for 90% variance)\n",
    "# You can adjust variance_threshold to 0.95 for stricter precision, or 0.85 for speed\n",
    "best_k = find_optimal_k(X_tfidf_matrix, variance_threshold=0.90)\n",
    "\n",
    "# Apply TruncatedSVD with the calculated best_k\n",
    "print(f\"\\nApplying TruncatedSVD with K={best_k}...\")\n",
    "svd_final = TruncatedSVD(n_components=best_k, random_state=42)\n",
    "\n",
    "X_tfidf_reduced_matrix = svd_final.fit_transform(X_tfidf_matrix)\n",
    "\n",
    "print(f\"Final Reduced Matrix Shape: {X_tfidf_reduced_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af2223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# SHAPE THE TRAINING DATA: Normalize to Unit Length\n",
    "# This converts Euclidean distance into Cosine Similarity logic.\n",
    "# It prevents \"Big\" clusters from swallowing \"Small\" ones due to magnitude.\n",
    "normalizer = Normalizer()\n",
    "X_tfidf_matrix_normalized = normalizer.fit_transform(X_tfidf_reduced_matrix)\n",
    "\n",
    "print(\"\\nData normalized for Spherical K-Means...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a23ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "# Initialize empty lists to store the evaluation metrics\n",
    "db_scores = []\n",
    "sil_scores = []\n",
    "\n",
    "# Use the specific data from your pipeline (Normalized + SVD)\n",
    "X_data = X_tfidf_matrix_normalized.copy()\n",
    "\n",
    "print(\"Evaluating clusters from K=2 to K=10...\\n\")\n",
    "\n",
    "# Iterate through a range of potential cluster numbers (k) from 2 to 10\n",
    "for k in range(2, 10):\n",
    "    # Initialize KMeans with your preferred random_state\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=43, \n",
    "        n_init=10,        # High number helps find small clusters\n",
    "        max_iter=100,     # Give it more time to converge on complex boundaries\n",
    "        init='k-means++'\n",
    "    )\n",
    "\n",
    "    # Fit the model and predict labels\n",
    "    labels = kmeans.fit_predict(X_data)\n",
    "    \n",
    "    # Calculate and store Davies-Bouldin Index (Lower is better)\n",
    "    db_index = davies_bouldin_score(X_data, labels)\n",
    "    db_scores.append(db_index)\n",
    "    \n",
    "    # Calculate and store Silhouette Score (Higher is better)\n",
    "    sil_score = silhouette_score(X_data, labels)\n",
    "    sil_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"K={k}: Davies-Bouldin={db_index:.4f} | Silhouette={sil_score:.4f}\")\n",
    "\n",
    "# Create a figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Davies-Bouldin scores (Red line with circles)\n",
    "ax.plot(range(2, 10), db_scores, marker='o', label='Davies-Bouldin', color='red')\n",
    "\n",
    "# Plot Silhouette scores (Blue line with crosses)\n",
    "ax.plot(range(2, 10), sil_scores, marker='x', label='Silhouette', color='blue')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Number of clusters (k)')\n",
    "ax.set_ylabel('Score')\n",
    "# ax.set_title('KMeans Cluster Validation Metrics')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "# Add a grid for easier reading of values\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"kmeans_clusters_dbouldin_silhouette.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=4,\n",
    "    random_state=43, \n",
    "    init='k-means++',  # Keep this fixed; it's almost always superior to 'random'\n",
    "    max_iter=100,      # Give it more time to converge on complex boundaries\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_init': [10, 30, 50],          # Tries 10, 30, or 50 restarts\n",
    "}\n",
    "\n",
    "# Set up Grid Search\n",
    "# Note: cv=3 is used because clustering CV is computationally expensive.\n",
    "# n_jobs=-1 uses all CPU cores.\n",
    "k_grid_search = GridSearchCV(\n",
    "    kmeans, \n",
    "    param_grid=param_grid, \n",
    "    scoring=make_scorer(silhouette_score),    \n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Running Optimized Grid Search...\")\n",
    "print(\"This may take some time as we are testing multiple parameters...\\n\")\n",
    "\n",
    "# Fit on Normalized Data\n",
    "# We fit on the NORMALIZED SVD matrix (Dense)\n",
    "k_grid_search.fit(X_tfidf_matrix_normalized)\n",
    "\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(k_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Final Model\n",
    "best_kmeans = k_grid_search.best_estimator_\n",
    "\n",
    "# Convert sparse TF-IDF to a dense DataFrame (optional: keep sparse for memory efficiency)\n",
    "k_results_df = pd.DataFrame(X_tfidf_matrix_normalized)\n",
    "\n",
    "# Add cluster labels\n",
    "k_results_df['cluster_label'] = best_kmeans.labels_\n",
    "\n",
    "# Add true original website labels (Category)\n",
    "k_results_df['true_label'] = y\n",
    "\n",
    "# Print how clusters correspond to categories\n",
    "print(\"\\nCluster Composition:\")\n",
    "print(k_results_df.groupby('cluster_label')['true_label'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Final Model\n",
    "best_kmeans = k_grid_search.best_estimator_\n",
    "\n",
    "# Convert sparse TF-IDF to a dense DataFrame (optional: keep sparse for memory efficiency)\n",
    "k_results_df = pd.DataFrame(X_tfidf_matrix_normalized)\n",
    "\n",
    "# Add cluster labels\n",
    "k_results_df['cluster_label'] = best_kmeans.labels_\n",
    "\n",
    "# Add true original website labels (Category)\n",
    "k_results_df['true_label'] = y\n",
    "\n",
    "k_results_df['topics'] = t\n",
    "\n",
    "# Group by the three dimensions and count the occurrences\n",
    "composition = k_results_df.groupby(['cluster_label', 'true_label', 'topics']).size()\n",
    "\n",
    "print(\"\\nDetailed Cluster Composition (Cluster > Category > Topic):\")\n",
    "print(composition.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1cec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"\\nGenerating 3D Cluster Visualization with Plotly...\")\n",
    "\n",
    "# 1. Reduce dimensions to 3D\n",
    "svd = TruncatedSVD(n_components=3, random_state=43)\n",
    "X_svd = svd.fit_transform(X_tfidf_matrix_normalized)\n",
    "\n",
    "# 2. Create DataFrame\n",
    "plot_df = pd.DataFrame(X_svd, columns=['Component 1', 'Component 2', 'Component 3'])\n",
    "\n",
    "# FIX: Keep labels as INTEGERS (numbers), not strings\n",
    "# This allows us to use the Viridis gradient\n",
    "plot_df['Cluster'] = best_kmeans.labels_\n",
    "\n",
    "# 3. Plot using scatter_3d\n",
    "fig = px.scatter_3d(\n",
    "    plot_df,\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    z='Component 3',\n",
    "    color='Cluster',\n",
    "    title=f'Interactive 3D K-Means Clusters (k={best_kmeans.n_clusters})<br><sub>TruncatedSVD Projection of TF-IDF Matrix</sub>',\n",
    "    opacity=0.7,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    # FIX: Use color_continuous_scale for gradients like Viridis\n",
    "    color_continuous_scale='viridis'\n",
    ")\n",
    "\n",
    "# Update the layout for cleaner axis labels\n",
    "fig.update_traces(marker=dict(size=3, line=dict(width=0)))\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Component 1',\n",
    "        yaxis_title='Component 2',\n",
    "        zaxis_title='Component 3'\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=50) # Tight layout\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (the words/bigrams/trigrams) from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Determine the number of clusters\n",
    "n_clusters = best_kmeans.n_clusters\n",
    "\n",
    "# Set up the plot grid (2 rows, 2 columns for 4 clusters)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Top 20 Words per Cluster (by TF-IDF Score)', fontsize=20)\n",
    "\n",
    "# Flatten the axes array for easy iteration (axes[0], axes[1], etc.)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # 1. Get the indices of all documents belonging to this cluster\n",
    "    cluster_indices = np.where(best_kmeans.labels_ == i)[0]\n",
    "    \n",
    "    # 2. Select the rows from the ORIGINAL TF-IDF matrix corresponding to this cluster\n",
    "    # We use X_tfidf_matrix (before SVD) because we want the actual words, not the latent components\n",
    "    cluster_tfidf = X_tfidf_matrix[cluster_indices]\n",
    "    \n",
    "    # 3. Sum the TF-IDF scores for each word across all documents in the cluster\n",
    "    # .sum(axis=0) sums column-wise (per feature)\n",
    "    word_scores = np.array(cluster_tfidf.sum(axis=0)).flatten()\n",
    "    \n",
    "    # 4. Get the indices of the top 20 scores\n",
    "    # argsort sorts ascending, so we take the last 20 and reverse them\n",
    "    top_indices = word_scores.argsort()[-20:][::-1]\n",
    "    \n",
    "    # 5. Map indices back to words and get the scores\n",
    "    top_words = feature_names[top_indices]\n",
    "    top_scores = word_scores[top_indices]\n",
    "    \n",
    "    # 6. Create a DataFrame for Seaborn\n",
    "    df_plot = pd.DataFrame({\n",
    "        'Word': top_words,\n",
    "        'Score': top_scores\n",
    "    })\n",
    "    \n",
    "    # 7. Plot on the specific subplot axis\n",
    "    sns.barplot(\n",
    "        ax=axes[i], \n",
    "        data=df_plot, \n",
    "        x='Score', \n",
    "        y='Word', \n",
    "        palette='viridis'\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f'Cluster {i}', fontsize=14)\n",
    "    axes[i].set_xlabel('Total TF-IDF Score')\n",
    "    axes[i].set_ylabel('') # Hide y-label as it's obvious\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for the main title\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"kmeans_top_20_words_per_cluster.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7084e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top Words per Cluster (by TF-IDF Score):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # (Steps A, B, C, D from your original code remain the same)\n",
    "    cluster_indices = np.where(best_kmeans.labels_ == i)[0]\n",
    "    cluster_tfidf = X_tfidf_matrix[cluster_indices]\n",
    "    word_scores = np.array(cluster_tfidf.sum(axis=0)).flatten()\n",
    "    top_indices = word_scores.argsort()[-20:][::-1]\n",
    "    \n",
    "    top_words = feature_names[top_indices]\n",
    "    top_scores = word_scores[top_indices]\n",
    "    \n",
    "    # Text display logic\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    # Join words with commas for a compact view\n",
    "    print(\", \".join(top_words))\n",
    "    \n",
    "    # Optional: If you want words with their specific scores\n",
    "    # for word, score in zip(top_words, top_scores):\n",
    "    #     print(f\"  {word: <15} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calculate centroids for all clusters\n",
    "centroids = []\n",
    "for i in range(n_clusters):\n",
    "    mask = (best_kmeans.labels_ == i)\n",
    "    centroids.append(X_tfidf_matrix_normalized[mask].mean(axis=0))\n",
    "\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# 2. Compute similarity matrix between centroids\n",
    "centroid_similarity = cosine_similarity(centroids)\n",
    "\n",
    "# 3. Plot as a Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    centroid_similarity, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    xticklabels=[f'C{i}' for i in range(n_clusters)],\n",
    "    yticklabels=[f'C{i}' for i in range(n_clusters)],\n",
    "    vmin=0, vmax=1\n",
    ")\n",
    "# plt.title('Inter-Cluster Similarity (Topic Overlap)')\n",
    "\n",
    "# Change this to whatever folder name you want\n",
    "folder_name = \"./practical_assessment_adsah_6014_2_web_content_mining/images/\"\n",
    "file_name = \"kmeans_inter_cluster_similarity.png\"\n",
    "\n",
    "# Create the folder if it doesn't exist yet\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Combine them into a full path (handles Windows/Mac differences automatically)\n",
    "full_path = os.path.join(folder_name, file_name)\n",
    "\n",
    "plt.savefig(full_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(f\"\\nPlot successfully saved to: {full_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the clusters (e.g., 'Cluster 0', 'Cluster 1', etc.)\n",
    "cluster_labels = [f'Cluster {i}' for i in range(n_clusters)]\n",
    "\n",
    "# Convert the numpy array into a Pandas DataFrame\n",
    "# We use the labels for both the Index (rows) and Columns\n",
    "df_similarity = pd.DataFrame(\n",
    "    centroid_similarity, \n",
    "    index=cluster_labels, \n",
    "    columns=cluster_labels\n",
    ")\n",
    "\n",
    "# Round the values to 3 decimal places for readability\n",
    "df_similarity = df_similarity.round(3)\n",
    "\n",
    "# Print the text-based table\n",
    "print(\"\\nInter-Cluster Similarity Matrix:\")\n",
    "print(df_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a248c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifying potential Outliers (weakest links)...\\n\")\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    cluster_mask = (best_kmeans.labels_ == i)\n",
    "    cluster_docs = X_tfidf_matrix_normalized[cluster_mask]\n",
    "    \n",
    "    # Reshape the centroid to be 2D (1 row, n_features columns)\n",
    "    # .mean(axis=0) creates a 1D array. .reshape(1, -1) converts it to 2D.\n",
    "    centroid = cluster_docs.mean(axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(cluster_docs, centroid).flatten()\n",
    "    \n",
    "    # Find the document with the LOWEST similarity to the center\n",
    "    worst_doc_local_index = similarities.argsort()[0]\n",
    "    \n",
    "    original_df_indices = np.where(best_kmeans.labels_ == i)[0]\n",
    "    worst_original_index = original_df_indices[worst_doc_local_index]\n",
    "    \n",
    "    score = similarities[worst_doc_local_index]\n",
    "    \n",
    "    # Only print if the document is really far away (e.g., similarity < 0.1)\n",
    "    if score < 0.1: \n",
    "        title = df['Title'].iloc[worst_original_index]\n",
    "        print(f\"CLUSTER {i} Outlier (Score: {score:.4f}):\")\n",
    "        print(f\"  > {title}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e2743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
